{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring tf.transform # \n",
    "\n",
    "**Learning Objectives**\n",
    "1. Preprocess data and engineer new features using TfTransform. \n",
    "1. Create and deploy Apache Beam pipeline. \n",
    "1. Use processed data to train taxifare model locally then serve a prediction.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Introduction \n",
    "While Pandas is fine for experimenting, for operationalization of your workflow it is better to do preprocessing in Apache Beam. This will also help if you need to preprocess data in flight, since Apache Beam allows for streaming. In this lab we will pull data from BigQuery then use Apache Beam  TfTransform to process the data.  \n",
    "\n",
    "Only specific combinations of TensorFlow/Beam are supported by tf.transform so make sure to get a combo that works. In this lab we will be using: \n",
    "* TFT 0.24.0\n",
    "* TF 2.3.0 \n",
    "* Apache Beam [GCP] 2.24.0\n",
    "\n",
    "Each learning objective will correspond to a __#TODO__ in the notebook where you will complete the notebook cell's code before running. Refer to the [solution notebook](../solutions/5_tftransform_taxifare.ipynb) for reference. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo chown -R jupyter:jupyter /home/jupyter/training-data-analyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-core in /opt/conda/lib/python3.7/site-packages (1.7.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-core) (1.16.0)\n",
      "Requirement already satisfied: google-api-core<3.0.0dev,>=1.21.0 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-core) (1.34.0)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.24.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-core) (1.35.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core<3.0.0dev,>=1.21.0->google-cloud-core) (1.56.4)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<3.0.0dev,>=1.21.0->google-cloud-core) (2.28.1)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /opt/conda/lib/python3.7/site-packages (from google-api-core<3.0.0dev,>=1.21.0->google-cloud-core) (3.19.6)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.24.0->google-cloud-core) (4.9)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.24.0->google-cloud-core) (59.8.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.24.0->google-cloud-core) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.24.0->google-cloud-core) (0.2.7)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.24.0->google-cloud-core) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3.0.0dev,>=1.21.0->google-cloud-core) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3.0.0dev,>=1.21.0->google-cloud-core) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3.0.0dev,>=1.21.0->google-cloud-core) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3.0.0dev,>=1.21.0->google-cloud-core) (3.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install google-cloud-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.3.0 in /opt/conda/lib/python3.7/site-packages (2.3.0)\n",
      "Requirement already satisfied: tensorflow-transform==0.24.0 in /opt/conda/lib/python3.7/site-packages (0.24.0)\n",
      "Requirement already satisfied: apache-beam[gcp]==2.24.0 in /opt/conda/lib/python3.7/site-packages (2.24.0)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.3.0) (2.10.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.3.0) (2.1.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.3.0) (2.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.3.0) (1.14.1)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.3.0) (2.10.1)\n",
      "Requirement already satisfied: gast==0.3.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.3.0) (0.3.3)\n",
      "Requirement already satisfied: scipy==1.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.3.0) (1.4.1)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.3.0) (3.19.6)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.3.0) (0.37.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.3.0) (3.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.3.0) (1.16.0)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.3.0) (1.18.5)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.3.0) (0.10.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.3.0) (1.6.3)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.3.0) (1.1.2)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.3.0) (1.50.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.3.0) (0.2.0)\n",
      "Requirement already satisfied: tfx-bsl<0.25,>=0.24 in /opt/conda/lib/python3.7/site-packages (from tensorflow-transform==0.24.0) (0.24.1)\n",
      "Requirement already satisfied: pydot<2,>=1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow-transform==0.24.0) (1.4.2)\n",
      "Requirement already satisfied: tensorflow-metadata<0.25,>=0.24 in /opt/conda/lib/python3.7/site-packages (from tensorflow-transform==0.24.0) (0.24.0)\n",
      "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.24.0) (2.7.0)\n",
      "Requirement already satisfied: mock<3.0.0,>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.24.0) (2.0.0)\n",
      "Requirement already satisfied: typing-extensions<3.8.0,>=3.7.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.24.0) (3.7.4.3)\n",
      "Requirement already satisfied: crcmod<2.0,>=1.7 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.24.0) (1.7)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.24.0) (2.8.2)\n",
      "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.24.0) (0.3.1.1)\n",
      "Requirement already satisfied: pytz>=2018.3 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.24.0) (2022.5)\n",
      "Requirement already satisfied: oauth2client<4,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.24.0) (3.0.0)\n",
      "Requirement already satisfied: future<1.0.0,>=0.18.2 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.24.0) (0.18.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.24.0) (2.28.1)\n",
      "Requirement already satisfied: httplib2<0.18.0,>=0.8 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.24.0) (0.17.4)\n",
      "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.24.0) (3.12.3)\n",
      "Requirement already satisfied: avro-python3!=1.9.2,<1.10.0,>=1.8.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.24.0) (1.9.2.1)\n",
      "Requirement already satisfied: pyarrow<0.18.0,>=0.15.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.24.0) (0.17.1)\n",
      "Requirement already satisfied: fastavro<0.24,>=0.21.4 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.24.0) (0.23.6)\n",
      "Requirement already satisfied: google-apitools<0.5.32,>=0.5.31 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.24.0) (0.5.31)\n",
      "Requirement already satisfied: google-cloud-datastore<2,>=1.7.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.24.0) (1.15.5)\n",
      "Requirement already satisfied: google-cloud-videointelligence<2,>=1.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.24.0) (1.16.3)\n",
      "Requirement already satisfied: google-auth<2,>=1.18.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.24.0) (1.35.0)\n",
      "Requirement already satisfied: google-cloud-pubsub<2,>=0.39.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.24.0) (1.7.2)\n",
      "Requirement already satisfied: google-cloud-spanner<2,>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.24.0) (1.19.3)\n",
      "Requirement already satisfied: google-cloud-language<2,>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.24.0) (1.3.2)\n",
      "Requirement already satisfied: google-cloud-bigquery<2,>=1.6.0 in /home/jupyter/.local/lib/python3.7/site-packages (from apache-beam[gcp]==2.24.0) (1.25.0)\n",
      "Requirement already satisfied: google-cloud-core<2,>=0.28.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.24.0) (1.7.3)\n",
      "Requirement already satisfied: google-cloud-dlp<2,>=0.12.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.24.0) (1.0.2)\n",
      "Requirement already satisfied: google-cloud-bigtable<2,>=0.31.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.24.0) (1.7.3)\n",
      "Requirement already satisfied: google-cloud-vision<2,>=0.38.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.24.0) (1.0.2)\n",
      "Requirement already satisfied: cachetools<4,>=3.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.24.0) (3.1.1)\n",
      "Requirement already satisfied: grpcio-gcp<1,>=0.2.2 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.24.0) (0.2.2)\n",
      "Requirement already satisfied: fasteners>=0.14 in /opt/conda/lib/python3.7/site-packages (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]==2.24.0) (0.18)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.18.0->apache-beam[gcp]==2.24.0) (59.8.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.18.0->apache-beam[gcp]==2.24.0) (0.2.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.18.0->apache-beam[gcp]==2.24.0) (4.9)\n",
      "Requirement already satisfied: google-api-core<2.0dev,>=1.15.0 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-bigquery<2,>=1.6.0->apache-beam[gcp]==2.24.0) (1.34.0)\n",
      "Requirement already satisfied: google-resumable-media<0.6dev,>=0.5.0 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-bigquery<2,>=1.6.0->apache-beam[gcp]==2.24.0) (0.5.1)\n",
      "Requirement already satisfied: grpc-google-iam-v1<0.13dev,>=0.12.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigtable<2,>=0.31.1->apache-beam[gcp]==2.24.0) (0.12.4)\n",
      "Requirement already satisfied: docopt in /opt/conda/lib/python3.7/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]==2.24.0) (0.6.2)\n",
      "Requirement already satisfied: pbr>=0.11 in /opt/conda/lib/python3.7/site-packages (from mock<3.0.0,>=1.0.1->apache-beam[gcp]==2.24.0) (5.11.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /opt/conda/lib/python3.7/site-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]==2.24.0) (0.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /opt/conda/lib/python3.7/site-packages (from pydot<2,>=1.2->tensorflow-transform==0.24.0) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]==2.24.0) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]==2.24.0) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]==2.24.0) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]==2.24.0) (2.1.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.4.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.8.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.2.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-metadata<0.25,>=0.24->tensorflow-transform==0.24.0) (1.56.4)\n",
      "Requirement already satisfied: google-api-python-client<2,>=1.7.11 in /opt/conda/lib/python3.7/site-packages (from tfx-bsl<0.25,>=0.24->tensorflow-transform==0.24.0) (1.8.0)\n",
      "Requirement already satisfied: tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,<3,>=1.15 in /opt/conda/lib/python3.7/site-packages (from tfx-bsl<0.25,>=0.24->tensorflow-transform==0.24.0) (2.3.0)\n",
      "Requirement already satisfied: pandas<2,>=1.0 in /opt/conda/lib/python3.7/site-packages (from tfx-bsl<0.25,>=0.24->tensorflow-transform==0.24.0) (1.3.5)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery<2,>=1.6.0->apache-beam[gcp]==2.24.0) (1.48.2)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.11->tfx-bsl<0.25,>=0.24->tensorflow-transform==0.24.0) (3.0.1)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.11->tfx-bsl<0.25,>=0.24->tensorflow-transform==0.24.0) (0.1.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.11.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=1.0.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.10.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.3.0 tensorflow-transform==0.24.0 apache-beam[gcp]==2.24.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**:Â You may ignore specific incompatibility errors and warnings. These components and issues do not impact your ability to complete the lab.\n",
    "Download .whl file for tensorflow-transform. We will pass this file to Beam Pipeline Options so it is installed on the DataFlow workers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-bigquery==1.25.0 in /home/jupyter/.local/lib/python3.7/site-packages (1.25.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==1.25.0) (3.19.6)\n",
      "Requirement already satisfied: six<2.0.0dev,>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==1.25.0) (1.16.0)\n",
      "Requirement already satisfied: google-resumable-media<0.6dev,>=0.5.0 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-bigquery==1.25.0) (0.5.1)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==1.25.0) (1.7.3)\n",
      "Requirement already satisfied: google-api-core<2.0dev,>=1.15.0 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-bigquery==1.25.0) (1.34.0)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==1.25.0) (1.35.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery==1.25.0) (1.56.4)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery==1.25.0) (2.28.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.9.0->google-cloud-bigquery==1.25.0) (4.9)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.9.0->google-cloud-bigquery==1.25.0) (59.8.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.9.0->google-cloud-bigquery==1.25.0) (3.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.9.0->google-cloud-bigquery==1.25.0) (0.2.7)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.9.0->google-cloud-bigquery==1.25.0) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery==1.25.0) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery==1.25.0) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery==1.25.0) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery==1.25.0) (1.26.11)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user google-cloud-bigquery==1.25.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-transform==0.24.0\n",
      "  File was already downloaded /home/jupyter/training-data-analyst/courses/machine_learning/deepdive2/feature_engineering/labs/tensorflow_transform-0.24.0-py3-none-any.whl\n",
      "Successfully downloaded tensorflow-transform\n"
     ]
    }
   ],
   "source": [
    "!pip download tensorflow-transform==0.24.0 --no-deps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Restart the kernel</b> (click on the reload button above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apache-beam==2.24.0\n",
      "tensorflow==2.3.0\n",
      "tensorflow-cloud==0.1.16\n",
      "tensorflow-datasets==4.7.0\n",
      "tensorflow-estimator==2.3.0\n",
      "tensorflow-hub==0.12.0\n",
      "tensorflow-io==0.27.0\n",
      "tensorflow-io-gcs-filesystem==0.27.0\n",
      "tensorflow-metadata==0.24.0\n",
      "tensorflow-probability==0.18.0\n",
      "tensorflow-serving-api==2.3.0\n",
      "tensorflow-transform==0.24.0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip freeze | grep -e 'flow\\|beam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-03 11:22:50.614144: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-12-03 11:22:50.614192: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import shutil\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change these to try this notebook out\n",
    "PROJECT = 'qwiklabs-gcp-02-7d72e1a2a255'\n",
    "BUCKET = PROJECT\n",
    "#REGION = 'us-west1-b'\n",
    "REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if ! gsutil ls | grep -q gs://${BUCKET}/; then\n",
    "  gsutil mb -l ${REGION} gs://${BUCKET}\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input source: BigQuery\n",
    "\n",
    "Get data from BigQuery but defer the majority of filtering etc. to Beam.\n",
    "Note that the dayofweek column is now strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "\n",
    "def create_query(phase, EVERY_N):\n",
    "    \"\"\"Creates a query with the proper splits.\n",
    "\n",
    "    Args:\n",
    "        phase: int, 1=train, 2=valid.\n",
    "        EVERY_N: int, take an example EVERY_N rows.\n",
    "\n",
    "    Returns:\n",
    "        Query string with the proper splits.\n",
    "    \"\"\"\n",
    "    base_query = \"\"\"\n",
    "    WITH daynames AS\n",
    "    (SELECT ['Sun', 'Mon', 'Tues', 'Wed', 'Thurs', 'Fri', 'Sat'] AS daysofweek)\n",
    "    SELECT\n",
    "    (tolls_amount + fare_amount) AS fare_amount,\n",
    "    daysofweek[ORDINAL(EXTRACT(DAYOFWEEK FROM pickup_datetime))] AS dayofweek,\n",
    "    EXTRACT(HOUR FROM pickup_datetime) AS hourofday,\n",
    "    pickup_longitude AS pickuplon,\n",
    "    pickup_latitude AS pickuplat,\n",
    "    dropoff_longitude AS dropofflon,\n",
    "    dropoff_latitude AS dropofflat,\n",
    "    passenger_count AS passengers,\n",
    "    'notneeded' AS key\n",
    "    FROM\n",
    "    `nyc-tlc.yellow.trips`, daynames\n",
    "    WHERE\n",
    "    trip_distance > 0 AND fare_amount > 0\n",
    "    \"\"\"\n",
    "    if EVERY_N is None:\n",
    "        if phase < 2:\n",
    "            # training\n",
    "            query = \"\"\"{0} AND ABS(MOD(FARM_FINGERPRINT(CAST\n",
    "            (pickup_datetime AS STRING), 4)) < 2\"\"\".format(base_query)\n",
    "        else:\n",
    "            query = \"\"\"{0} AND ABS(MOD(FARM_FINGERPRINT(CAST(\n",
    "            pickup_datetime AS STRING), 4)) = {1}\"\"\".format(base_query, phase)\n",
    "    else:\n",
    "        query = \"\"\"{0} AND ABS(MOD(FARM_FINGERPRINT(CAST(\n",
    "        pickup_datetime AS STRING)), {1})) = {2}\"\"\".format(\n",
    "            base_query, EVERY_N, phase)\n",
    "\n",
    "    return query\n",
    "\n",
    "query = create_query(2, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /opt/conda/lib/python3.7/site-packages (0.17.1)\n",
      "Requirement already satisfied: numpy>=1.14 in /opt/conda/lib/python3.7/site-packages (from pyarrow) (1.18.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    WITH daynames AS\n",
      "    (SELECT ['Sun', 'Mon', 'Tues', 'Wed', 'Thurs', 'Fri', 'Sat'] AS daysofweek)\n",
      "    SELECT\n",
      "    (tolls_amount + fare_amount) AS fare_amount,\n",
      "    daysofweek[ORDINAL(EXTRACT(DAYOFWEEK FROM pickup_datetime))] AS dayofweek,\n",
      "    EXTRACT(HOUR FROM pickup_datetime) AS hourofday,\n",
      "    pickup_longitude AS pickuplon,\n",
      "    pickup_latitude AS pickuplat,\n",
      "    dropoff_longitude AS dropofflon,\n",
      "    dropoff_latitude AS dropofflat,\n",
      "    passenger_count AS passengers,\n",
      "    'notneeded' AS key\n",
      "    FROM\n",
      "    `nyc-tlc.yellow.trips`, daynames\n",
      "    WHERE\n",
      "    trip_distance > 0 AND fare_amount > 0\n",
      "     AND ABS(MOD(FARM_FINGERPRINT(CAST(\n",
      "        pickup_datetime AS STRING)), 100000)) = 2\n"
     ]
    }
   ],
   "source": [
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: google-api-python-client 1.12.11\n",
      "Uninstalling google-api-python-client-1.12.11:\n",
      "  Successfully uninstalled google-api-python-client-1.12.11\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-api-python-client==1.8.0 in /opt/conda/lib/python3.7/site-packages (1.8.0)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client==1.8.0) (3.0.1)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client==1.8.0) (0.1.0)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client==1.8.0) (0.17.4)\n",
      "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /home/jupyter/.local/lib/python3.7/site-packages (from google-api-python-client==1.8.0) (1.34.0)\n",
      "Requirement already satisfied: six<2dev,>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client==1.8.0) (1.16.0)\n",
      "Requirement already satisfied: google-auth>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client==1.8.0) (1.35.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0) (3.19.6)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0) (2.28.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0) (1.56.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0) (4.9)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0) (3.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0) (0.2.7)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0) (59.8.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.4.1->google-api-python-client==1.8.0) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0) (3.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install google-api-python-client==1.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyarrow.vendored.version'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16769/1559023655.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \"\"\"\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sql\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/bigquery/job/query.py\u001b[0m in \u001b[0;36mto_dataframe\u001b[0;34m(self, bqstorage_client, dtypes, progress_bar_type, create_bqstorage_client, date_as_object, max_results, geography_as_object)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/bigquery/table.py\u001b[0m in \u001b[0;36mto_dataframe\u001b[0;34m(self, bqstorage_client, dtypes, progress_bar_type, create_bqstorage_client, date_as_object, geography_as_object)\u001b[0m\n\u001b[1;32m   2009\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2011\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2012\u001b[0m         \u001b[0mkey_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"{}={}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2013\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"RangePartitioning({})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._PandasConvertible.to_pandas\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/table.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.Table._to_pandas\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/pandas_compat.py\u001b[0m in \u001b[0;36mtable_to_blockmanager\u001b[0;34m(options, table, categories, ignore_metadata, types_mapper)\u001b[0m\n\u001b[1;32m    759\u001b[0m             table, all_columns, types_mapper)\n\u001b[1;32m    760\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pandas_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRangeIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m         \u001b[0mext_columns_dtypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_extension_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes_mapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/pandas-shim.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._PandasAPIShim.pd.__get__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/pandas-shim.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._PandasAPIShim._check_import\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/pandas-shim.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._PandasAPIShim._import_pandas\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyarrow.vendored.version'"
     ]
    }
   ],
   "source": [
    "# https://cloud.google.com/bigquery/docs/samples/bigquery-client-default-credentials#bigquery_client_default_credentials-python\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# If you don't specify credentials when constructing the client, the\n",
    "# client library will look for credentials in the environment.\n",
    "client = bigquery.Client()\n",
    "\n",
    "# https://cloud.google.com/bigquery/docs/samples/bigquery-query-results-dataframe\n",
    "test_sql = \"\"\"\n",
    "    SELECT name, SUM(number) as count\n",
    "    FROM `bigquery-public-data.usa_names.usa_1910_current`\n",
    "    GROUP BY name\n",
    "    ORDER BY count DESC\n",
    "    LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "df = client.query(test_sql).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyarrow.vendored.version'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16769/2167275648.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbigquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/bigquery/job/query.py\u001b[0m in \u001b[0;36mto_dataframe\u001b[0;34m(self, bqstorage_client, dtypes, progress_bar_type, create_bqstorage_client, date_as_object, max_results, geography_as_object)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/bigquery/table.py\u001b[0m in \u001b[0;36mto_dataframe\u001b[0;34m(self, bqstorage_client, dtypes, progress_bar_type, create_bqstorage_client, date_as_object, geography_as_object)\u001b[0m\n\u001b[1;32m   2009\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2011\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2012\u001b[0m         \u001b[0mkey_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"{}={}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2013\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"RangePartitioning({})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._PandasConvertible.to_pandas\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/table.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.Table._to_pandas\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/pandas_compat.py\u001b[0m in \u001b[0;36mtable_to_blockmanager\u001b[0;34m(options, table, categories, ignore_metadata, types_mapper)\u001b[0m\n\u001b[1;32m    759\u001b[0m             table, all_columns, types_mapper)\n\u001b[1;32m    760\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pandas_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRangeIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m         \u001b[0mext_columns_dtypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_extension_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes_mapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/pandas-shim.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._PandasAPIShim.pd.__get__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/pandas-shim.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._PandasAPIShim._check_import\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/pandas-shim.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._PandasAPIShim._import_pandas\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyarrow.vendored.version'"
     ]
    }
   ],
   "source": [
    "df_valid = bigquery.Client().query(query).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.head()\n",
    "df_valid.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ML dataset using tf.transform and Dataflow\n",
    "\n",
    "Let's use Cloud Dataflow to read in the BigQuery data and write it out as TFRecord files. Along the way, let's use tf.transform to do scaling and transforming. Using tf.transform allows us to save the metadata to ensure that the appropriate transformations get carried out during prediction as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`transformed_data` is type `pcollection`.\n",
    "\n",
    "5 __TODO's__ in the following cell block \n",
    "1. Convert day of week from string->int with `tft.string_to_int`\n",
    "1. Scale `pickuplat`, `pickuplon`, `dropofflat`, `dropofflon` between 0 and 1 with `tft.scale_to_0_1`\n",
    "1. Scale our engineered features `latdiff` and `londiff` between 0 and 1\n",
    "1. Analyze and transform our training data using `beam_impl.AnalyzeAndTransformDataset()`\n",
    "1. Read eval data from BigQuery using `beam.io.BigQuerySource` and filter rows using our `is_valid` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: pyarrow.lib.Message size changed, may indicate binary incompatibility. Expected 24 from C header, got 32 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: pyarrow.lib.MemoryPool size changed, may indicate binary incompatibility. Expected 32 from C header, got 40 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: pyarrow.lib.DictionaryMemo size changed, may indicate binary incompatibility. Expected 40 from C header, got 48 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: pyarrow.lib._Metadata size changed, may indicate binary incompatibility. Expected 16 from C header, got 24 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: pyarrow.lib.KeyValueMetadata size changed, may indicate binary incompatibility. Expected 48 from C header, got 56 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: pyarrow.lib.Field size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: pyarrow.lib.Schema size changed, may indicate binary incompatibility. Expected 48 from C header, got 56 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: pyarrow.lib.Scalar size changed, may indicate binary incompatibility. Expected 24 from C header, got 48 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'pyarrow.lib' has no attribute 'NAType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16769/1216217266.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mapache_beam\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_transform\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtft\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_metadata\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfmd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/apache_beam/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mapache_beam\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcoders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mapache_beam\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mapache_beam\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypehints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mapache_beam\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/apache_beam/io/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mapache_beam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miobase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mapache_beam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmongodbio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mapache_beam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquetio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mapache_beam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mapache_beam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtfrecordio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/apache_beam/io/parquetio.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m   \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m   \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0mpa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/parquet.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parquet\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_parquet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m from pyarrow._parquet import (ParquetReader, Statistics,  # noqa\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/lib.pxd\u001b[0m in \u001b[0;36minit pyarrow._parquet\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pyarrow.lib' has no attribute 'NAType'"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import tensorflow as tf\n",
    "import apache_beam as beam\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow_metadata as tfmd\n",
    "from tensorflow_transform.beam import impl as beam_impl\n",
    "\n",
    "\n",
    "def is_valid(inputs):\n",
    "    \"\"\"Check to make sure the inputs are valid.\n",
    "\n",
    "    Args:\n",
    "        inputs: dict, dictionary of TableRow data from BigQuery.\n",
    "\n",
    "    Returns:\n",
    "        True if the inputs are valid and False if they are not.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pickup_longitude = inputs['pickuplon']\n",
    "        dropoff_longitude = inputs['dropofflon']\n",
    "        pickup_latitude = inputs['pickuplat']\n",
    "        dropoff_latitude = inputs['dropofflat']\n",
    "        hourofday = inputs['hourofday']\n",
    "        dayofweek = inputs['dayofweek']\n",
    "        passenger_count = inputs['passengers']\n",
    "        fare_amount = inputs['fare_amount']\n",
    "        return fare_amount >= 2.5 and pickup_longitude > -78 \\\n",
    "            and pickup_longitude < -70 and dropoff_longitude > -78 \\\n",
    "            and dropoff_longitude < -70 and pickup_latitude > 37 \\\n",
    "            and pickup_latitude < 45 and dropoff_latitude > 37 \\\n",
    "            and dropoff_latitude < 45 and passenger_count > 0\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take __10-15 minutes__. You cannot go on in this lab until your DataFlow job has successfully completed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tft(inputs):\n",
    "    \"\"\"Preprocess the features and add engineered features with tf transform.\n",
    "\n",
    "    Args:\n",
    "        dict, dictionary of TableRow data from BigQuery.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of preprocessed data after scaling and feature engineering.\n",
    "    \"\"\"\n",
    "    import datetime\n",
    "    print(inputs)\n",
    "    result = {}\n",
    "    result['fare_amount'] = tf.identity(inputs['fare_amount'])\n",
    "    # Build a vocabulary\n",
    "\n",
    "    # TODO 1: convert day of week from string->int with tft.string_to_int\n",
    "\n",
    "    result['hourofday'] = tf.identity(inputs['hourofday'])  # pass through\n",
    "\n",
    "    # TODO 2: scale pickup/dropoff lat/lon between 0 and 1 with tft.scale_to_0_1\n",
    "\n",
    "    result['passengers'] = tf.cast(inputs['passengers'], tf.float32)  # a cast\n",
    "    # Arbitrary TF func\n",
    "    result['key'] = tf.as_string(tf.ones_like(inputs['passengers']))\n",
    "\n",
    "    # Engineered features\n",
    "    latdiff = inputs['pickuplat'] - inputs['dropofflat']\n",
    "    londiff = inputs['pickuplon'] - inputs['dropofflon']\n",
    "\n",
    "    # TODO 3: Scale our engineered features latdiff and londiff between 0 and 1\n",
    "\n",
    "    dist = tf.sqrt(latdiff * latdiff + londiff * londiff)\n",
    "    result['euclidean'] = tft.scale_to_0_1(dist)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(in_test_mode):\n",
    "    \"\"\"Sets up preprocess pipeline.\n",
    "\n",
    "    Args:\n",
    "        in_test_mode: bool, False to launch DataFlow job, True to run locally.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import os.path\n",
    "    import tempfile\n",
    "    from apache_beam.io import tfrecordio\n",
    "    from tensorflow_transform.coders import example_proto_coder\n",
    "    from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "    from tensorflow_transform.tf_metadata import dataset_schema\n",
    "    from tensorflow_transform.beam import tft_beam_io\n",
    "    from tensorflow_transform.beam.tft_beam_io import transform_fn_io\n",
    "\n",
    "    job_name = 'preprocess-taxi-features' + '-'\n",
    "    job_name += datetime.datetime.now().strftime('%y%m%d-%H%M%S')\n",
    "    if in_test_mode:\n",
    "        import shutil\n",
    "        print('Launching local job ... hang on')\n",
    "        OUTPUT_DIR = './preproc_tft'\n",
    "        shutil.rmtree(OUTPUT_DIR, ignore_errors=True)\n",
    "        EVERY_N = 100000\n",
    "    else:\n",
    "        print('Launching Dataflow job {} ... hang on'.format(job_name))\n",
    "        OUTPUT_DIR = 'gs://{0}/taxifare/preproc_tft/'.format(BUCKET)\n",
    "        import subprocess\n",
    "        subprocess.call('gsutil rm -r {}'.format(OUTPUT_DIR).split())\n",
    "        EVERY_N = 10000\n",
    "\n",
    "    options = {\n",
    "        'staging_location': os.path.join(OUTPUT_DIR, 'tmp', 'staging'),\n",
    "        'temp_location': os.path.join(OUTPUT_DIR, 'tmp'),\n",
    "        'job_name': job_name,\n",
    "        'project': PROJECT,\n",
    "        'num_workers': 1,\n",
    "        'max_num_workers': 1,\n",
    "        'teardown_policy': 'TEARDOWN_ALWAYS',\n",
    "        'no_save_main_session': True,\n",
    "        'direct_num_workers': 1,\n",
    "        'extra_packages': ['tensorflow_transform-0.24.0-py3-none-any.whl']\n",
    "        }\n",
    "\n",
    "    opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "    if in_test_mode:\n",
    "        RUNNER = 'DirectRunner'\n",
    "    else:\n",
    "        RUNNER = 'DataflowRunner'\n",
    "\n",
    "    # Set up raw data metadata\n",
    "    raw_data_schema = {\n",
    "        colname: dataset_schema.ColumnSchema(\n",
    "            tf.string, [], dataset_schema.FixedColumnRepresentation())\n",
    "        for colname in 'dayofweek,key'.split(',')\n",
    "    }\n",
    "\n",
    "    raw_data_schema.update({\n",
    "        colname: dataset_schema.ColumnSchema(\n",
    "            tf.float32, [], dataset_schema.FixedColumnRepresentation())\n",
    "        for colname in\n",
    "        'fare_amount,pickuplon,pickuplat,dropofflon,dropofflat'.split(',')\n",
    "    })\n",
    "\n",
    "    raw_data_schema.update({\n",
    "        colname: dataset_schema.ColumnSchema(\n",
    "            tf.int64, [], dataset_schema.FixedColumnRepresentation())\n",
    "        for colname in 'hourofday,passengers'.split(',')\n",
    "    })\n",
    "\n",
    "    raw_data_metadata = dataset_metadata.DatasetMetadata(\n",
    "        dataset_schema.Schema(raw_data_schema))\n",
    "\n",
    "    # Run Beam\n",
    "    with beam.Pipeline(RUNNER, options=opts) as p:\n",
    "        with beam_impl.Context(temp_dir=os.path.join(OUTPUT_DIR, 'tmp')):\n",
    "            # Save the raw data metadata\n",
    "            (raw_data_metadata |\n",
    "                'WriteInputMetadata' >> tft_beam_io.WriteMetadata(\n",
    "                    os.path.join(\n",
    "                        OUTPUT_DIR, 'metadata/rawdata_metadata'), pipeline=p))\n",
    "\n",
    "            # TODO 4: Analyze and transform our training data\n",
    "            # using beam_impl.AnalyzeAndTransformDataset()\n",
    "\n",
    "            raw_dataset = (raw_data, raw_data_metadata)\n",
    "\n",
    "            # Analyze and transform training data\n",
    "            transformed_dataset, transform_fn = (\n",
    "                raw_dataset | beam_impl.AnalyzeAndTransformDataset(\n",
    "                    preprocess_tft))\n",
    "            transformed_data, transformed_metadata = transformed_dataset\n",
    "\n",
    "            # Save transformed train data to disk in efficient tfrecord format\n",
    "            transformed_data | 'WriteTrainData' >> tfrecordio.WriteToTFRecord(\n",
    "                os.path.join(OUTPUT_DIR, 'train'), file_name_suffix='.gz',\n",
    "                coder=example_proto_coder.ExampleProtoCoder(\n",
    "                    transformed_metadata.schema))\n",
    "\n",
    "            # TODO 5: Read eval data from BigQuery using beam.io.BigQuerySource\n",
    "            # and filter rows using our is_valid function\n",
    "\n",
    "            raw_test_dataset = (raw_test_data, raw_data_metadata)\n",
    "\n",
    "            # Transform eval data\n",
    "            transformed_test_dataset = (\n",
    "                (raw_test_dataset, transform_fn) | beam_impl.TransformDataset()\n",
    "                )\n",
    "            transformed_test_data, _ = transformed_test_dataset\n",
    "\n",
    "            # Save transformed train data to disk in efficient tfrecord format\n",
    "            (transformed_test_data |\n",
    "                'WriteTestData' >> tfrecordio.WriteToTFRecord(\n",
    "                    os.path.join(OUTPUT_DIR, 'eval'), file_name_suffix='.gz',\n",
    "                    coder=example_proto_coder.ExampleProtoCoder(\n",
    "                        transformed_metadata.schema)))\n",
    "\n",
    "            # Save transformation function to disk for use at serving time\n",
    "            (transform_fn |\n",
    "                'WriteTransformFn' >> transform_fn_io.WriteTransformFn(\n",
    "                    os.path.join(OUTPUT_DIR, 'metadata')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to True to run locally\n",
    "preprocess(in_test_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The above command may fail with an error **`Workflow failed. Causes: There was a problem refreshing your credentials`**. In that case, `re-run` the command again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check to make sure that there is data where we expect it to be now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# ls preproc_tft\n",
    "gsutil ls gs://${BUCKET}/taxifare/preproc_tft/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train off preprocessed data ##\n",
    "Now that we have our data ready and verified it is in the correct location we can train our taxifare model locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -r ./taxi_trained\n",
    "export PYTHONPATH=${PYTHONPATH}:$PWD\n",
    "python3 -m tft_trainer.task \\\n",
    "    --train_data_path=\"gs://${BUCKET}/taxifare/preproc_tft/train*\" \\\n",
    "    --eval_data_path=\"gs://${BUCKET}/taxifare/preproc_tft/eval*\"  \\\n",
    "    --output_dir=./taxi_trained \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $PWD/taxi_trained/export/exporter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create fake data in JSON format and use it to serve a prediction with gcloud ai-platform local predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /tmp/test.json\n",
    "{\"dayofweek\":0, \"hourofday\":17, \"pickuplon\": -73.885262, \"pickuplat\": 40.773008, \"dropofflon\": -73.987232, \"dropofflat\": 40.732403, \"passengers\": 2.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "sudo find \"/usr/lib/google-cloud-sdk/lib/googlecloudsdk/command_lib/ml_engine\" -name '*.pyc' -delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "model_dir=$(ls $PWD/taxi_trained/export/exporter/)\n",
    "gcloud ai-platform local predict \\\n",
    "    --model-dir=./taxi_trained/export/exporter/${model_dir} \\\n",
    "    --json-instances=/tmp/test.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2022 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-10.m100",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-10:m100"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
