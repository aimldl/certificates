* Draft: 2022-11-26 (Sat)

# Design and Build a TensorFlow Input Data Pipeline

Data is the a crucial component of a machine learning model. Collecting the right data is not enough. You also need to make sure you put the right processes in place to clean, analyze and transform the data, as needed, so that the model can take the most signal of it as possible. In this module we discuss training on large datasets with tf.data, working with in-memory files, and how to get the data ready for training. Then we discuss embeddings, and end with an overview of scaling data with tf.keras preprocessing layers.

**Learning Objectives**

- Train on large datasets with tf.data
- Work with in-memory files
- Get the data ready for training
- Describe embeddings
- Understand scaling data with tf.Keras preprocessing layers

### Design and Build a TensorFlow Input Data Pipeline: Module Introduction

- [ ] Introduction, 22 sec
- [ ] An ML recap, 2 min

### Training on large datasets with tf.data API

- [ ] Training on large datasets with tf.data API, 3 min
- [ ] Working in-memory and with files, minutes3 min
- [ ] Getting the data ready for model training, 6 min
- [ ] Embeddings, 8 min
- [ ] Coursera: Getting Started with Google Cloud and Qwiklabs, 4 min
- [ ] Lab intro: TensorFlow Dataset API, 27 sec
- [ ] Lab: TensorFlow Dataset API

### Scaling data processing with tf.data and Keras preprocessing layers

- [ ] Scaling data processing with tf.data and Keras preprocessing layers, 10 min
  Lab intro: Classifying structured data using Keras preprocessing layers, 31 sec
  Lab: Classifying structured data using Keras preprocessing layers
- [ ] Overdue

- [ ] [Quiz] Design and Build Input Data Pipeline, 8 questions
- [ ] Resources: Design and Build a TensorFlow Input Data Pipeline, 10 min
