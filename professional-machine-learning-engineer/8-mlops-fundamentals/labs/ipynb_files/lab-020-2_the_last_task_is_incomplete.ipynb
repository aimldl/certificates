{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous training pipeline with Kubeflow Pipeline and AI Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Objectives:**\n",
    "1. Learn how to use Kubeflow Pipeline (KFP) pre-build components (BiqQuery, AI Platform training and predictions)\n",
    "1. Learn how to use KFP lightweight python components\n",
    "1. Learn how to build a KFP with these components\n",
    "1. Learn how to compile, upload, and run a KFP with the command line\n",
    "\n",
    "\n",
    "In this lab, you will build, deploy, and run a KFP pipeline that orchestrates **BigQuery** and **AI Platform** services to train, tune, and deploy a **scikit-learn** model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the pipeline design\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workflow implemented by the pipeline is defined using a Python based Domain Specific Language (DSL). The pipeline's DSL is in the `covertype_training_pipeline.py` file that we will generate below.\n",
    "\n",
    "The pipeline's DSL has been designed to avoid hardcoding any environment specific settings like file paths or connection strings. These settings are provided to the pipeline code through a set of environment variables.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "```bash\n",
    "grep: pipeline/covertype_training_pipeline.py: No such file or directory\n",
    "```\n",
    "This file is under directoy `/lab-02-kfp-pipeline/pipeline`.\n",
    "\n",
    "## Solution\n",
    "Copy the file to directory `/lab-02-kfp-pipeline/exercises/pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_IMAGE = os.getenv('BASE_IMAGE')\n",
      "TRAINER_IMAGE = os.getenv('TRAINER_IMAGE')\n",
      "RUNTIME_VERSION = os.getenv('RUNTIME_VERSION')\n",
      "PYTHON_VERSION = os.getenv('PYTHON_VERSION')\n",
      "COMPONENT_URL_SEARCH_PREFIX = os.getenv('COMPONENT_URL_SEARCH_PREFIX')\n",
      "USE_KFP_SA = os.getenv('USE_KFP_SA')\n"
     ]
    }
   ],
   "source": [
    "!grep 'BASE_IMAGE =' -A 5 pipeline/covertype_training_pipeline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: Because there are no environment variables set, therefore covertype_training_pipeline.py file is missing;  we will create it in the next step.**\n",
    "\n",
    "The pipeline uses a mix of custom and pre-build components.\n",
    "\n",
    "- Pre-build components. The pipeline uses the following pre-build components that are included with the KFP distribution:\n",
    "    - [BigQuery query component](https://github.com/kubeflow/pipelines/tree/0.2.5/components/gcp/bigquery/query)\n",
    "    - [AI Platform Training component](https://github.com/kubeflow/pipelines/tree/0.2.5/components/gcp/ml_engine/train)\n",
    "    - [AI Platform Deploy component](https://github.com/kubeflow/pipelines/tree/0.2.5/components/gcp/ml_engine/deploy)\n",
    "- Custom components. The pipeline uses two custom helper components that encapsulate functionality not available in any of the pre-build components. The components are implemented using the KFP SDK's [Lightweight Python Components](https://www.kubeflow.org/docs/pipelines/sdk/lightweight-python-components/) mechanism. The code for the components is in the `helper_components.py` file:\n",
    "    - **Retrieve Best Run**. This component retrieves a tuning metric and hyperparameter values for the best run of a AI Platform Training hyperparameter tuning job.\n",
    "    - **Evaluate Model**. This component evaluates a *sklearn* trained model using a provided metric and a testing dataset.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Complete TO DOs the pipeline file below.\n",
    "\n",
    "<ql-infobox><b>NOTE:</b> If you need help, you may take a look at the complete solution by navigating to **mlops-on-gcp > workshops > kfp-caip-sklearn > lab-02-kfp-pipeline** and opening **lab-02.ipynb**.\n",
    "</ql-infobox>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./pipeline/covertype_training_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pipeline/covertype_training_pipeline.py\n",
    "# Copyright 2019 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"KFP orchestrating BigQuery and Cloud AI Platform services.\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "from helper_components import evaluate_model\n",
    "from helper_components import retrieve_best_run\n",
    "from jinja2 import Template\n",
    "import kfp\n",
    "from kfp.components import func_to_container_op\n",
    "from kfp.dsl.types import Dict\n",
    "from kfp.dsl.types import GCPProjectID\n",
    "from kfp.dsl.types import GCPRegion\n",
    "from kfp.dsl.types import GCSPath\n",
    "from kfp.dsl.types import String\n",
    "from kfp.gcp import use_gcp_secret\n",
    "\n",
    "# Defaults and environment settings\n",
    "BASE_IMAGE = os.getenv('BASE_IMAGE')\n",
    "TRAINER_IMAGE = os.getenv('TRAINER_IMAGE')\n",
    "RUNTIME_VERSION = os.getenv('RUNTIME_VERSION')\n",
    "PYTHON_VERSION = os.getenv('PYTHON_VERSION')\n",
    "COMPONENT_URL_SEARCH_PREFIX = os.getenv('COMPONENT_URL_SEARCH_PREFIX')\n",
    "USE_KFP_SA = os.getenv('USE_KFP_SA')\n",
    "\n",
    "TRAINING_FILE_PATH = 'datasets/training/data.csv'\n",
    "VALIDATION_FILE_PATH = 'datasets/validation/data.csv'\n",
    "TESTING_FILE_PATH = 'datasets/testing/data.csv'\n",
    "\n",
    "# Parameter defaults\n",
    "SPLITS_DATASET_ID = 'splits'\n",
    "HYPERTUNE_SETTINGS = \"\"\"\n",
    "{\n",
    "    \"hyperparameters\":  {\n",
    "        \"goal\": \"MAXIMIZE\",\n",
    "        \"maxTrials\": 6,\n",
    "        \"maxParallelTrials\": 3,\n",
    "        \"hyperparameterMetricTag\": \"accuracy\",\n",
    "        \"enableTrialEarlyStopping\": True,\n",
    "        \"params\": [\n",
    "            {\n",
    "                \"parameterName\": \"max_iter\",\n",
    "                \"type\": \"DISCRETE\",\n",
    "                \"discreteValues\": [500, 1000]\n",
    "            },\n",
    "            {\n",
    "                \"parameterName\": \"alpha\",\n",
    "                \"type\": \"DOUBLE\",\n",
    "                \"minValue\": 0.0001,\n",
    "                \"maxValue\": 0.001,\n",
    "                \"scaleType\": \"UNIT_LINEAR_SCALE\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Helper functions\n",
    "def generate_sampling_query(source_table_name, num_lots, lots):\n",
    "    \"\"\"Prepares the data sampling query.\"\"\"\n",
    "\n",
    "    sampling_query_template = \"\"\"\n",
    "         SELECT *\n",
    "         FROM \n",
    "             `{{ source_table }}` AS cover\n",
    "         WHERE \n",
    "         MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(cover))), {{ num_lots }}) IN ({{ lots }})\n",
    "         \"\"\"\n",
    "    query = Template(sampling_query_template).render(\n",
    "        source_table=source_table_name, num_lots=num_lots, lots=str(lots)[1:-1])\n",
    "\n",
    "    return query\n",
    "\n",
    "\n",
    "# Create component factories\n",
    "#component_store = # TO DO: Complete the command\n",
    "\n",
    "#bigquery_query_op = # TO DO: Use the pre-build bigquery/query component\n",
    "#mlengine_train_op = # TO DO: Use the pre-build ml_engine/train\n",
    "#mlengine_deploy_op = # TO DO: Use the pre-build ml_engine/deploy component\n",
    "#retrieve_best_run_op = # TO DO: Package the retrieve_best_run function into a lightweight component\n",
    "#evaluate_model_op = # TO DO: Package the evaluate_model function into a lightweight component\n",
    "\n",
    "component_store = kfp.components.ComponentStore(\n",
    "    local_search_paths=None, url_search_prefixes=[COMPONENT_URL_SEARCH_PREFIX])\n",
    "\n",
    "bigquery_query_op = component_store.load_component('bigquery/query')\n",
    "mlengine_train_op = component_store.load_component('ml_engine/train')\n",
    "mlengine_deploy_op = component_store.load_component('ml_engine/deploy')\n",
    "retrieve_best_run_op = func_to_container_op(\n",
    "    retrieve_best_run, base_image=BASE_IMAGE)\n",
    "evaluate_model_op = func_to_container_op(evaluate_model, base_image=BASE_IMAGE)\n",
    "\n",
    "@kfp.dsl.pipeline(\n",
    "    name='Covertype Classifier Training',\n",
    "    description='The pipeline training and deploying the Covertype classifierpipeline_yaml'\n",
    ")\n",
    "def covertype_train(project_id,\n",
    "                    region,\n",
    "                    source_table_name,\n",
    "                    gcs_root,\n",
    "                    dataset_id,\n",
    "                    evaluation_metric_name,\n",
    "                    evaluation_metric_threshold,\n",
    "                    model_id,\n",
    "                    version_id,\n",
    "                    replace_existing_version,\n",
    "                    hypertune_settings=HYPERTUNE_SETTINGS,\n",
    "                    dataset_location='US'):\n",
    "    \"\"\"Orchestrates training and deployment of an sklearn model.\"\"\"\n",
    "\n",
    "    # Create the training split\n",
    "    query = generate_sampling_query(\n",
    "        source_table_name=source_table_name, num_lots=10, lots=[1, 2, 3, 4])\n",
    "\n",
    "    training_file_path = '{}/{}'.format(gcs_root, TRAINING_FILE_PATH)\n",
    "\n",
    "    create_training_split = bigquery_query_op(\n",
    "        query=query,\n",
    "        project_id=project_id,\n",
    "        dataset_id=dataset_id,\n",
    "        table_id='',\n",
    "        output_gcs_path=training_file_path,\n",
    "        dataset_location=dataset_location)\n",
    "\n",
    "    # Create the validation split\n",
    "    query = generate_sampling_query(\n",
    "        source_table_name=source_table_name, num_lots=10, lots=[8])\n",
    "\n",
    "    validation_file_path = '{}/{}'.format(gcs_root, VALIDATION_FILE_PATH)\n",
    "\n",
    "#    create_validation_split = # TODO - use the bigquery_query_op\n",
    "    create_validation_split = bigquery_query_op(\n",
    "        query=query,\n",
    "        project_id=project_id,\n",
    "        dataset_id=dataset_id,\n",
    "        table_id='',\n",
    "        output_gcs_path=validation_file_path,\n",
    "        dataset_location=dataset_location)\n",
    "\n",
    "    # Create the testing split\n",
    "    query = generate_sampling_query(\n",
    "        source_table_name=source_table_name, num_lots=10, lots=[9])\n",
    "\n",
    "    testing_file_path = '{}/{}'.format(gcs_root, TESTING_FILE_PATH)\n",
    "\n",
    "#    create_testing_split = # TO DO: Use the bigquery_query_op\n",
    "    create_testing_split = bigquery_query_op(\n",
    "        query=query,\n",
    "        project_id=project_id,\n",
    "        dataset_id=dataset_id,\n",
    "        table_id='',\n",
    "        output_gcs_path=testing_file_path,\n",
    "        dataset_location=dataset_location)\n",
    "\n",
    "    # Tune hyperparameters\n",
    "    tune_args = [\n",
    "        '--training_dataset_path',\n",
    "        create_training_split.outputs['output_gcs_path'],\n",
    "        '--validation_dataset_path',\n",
    "        create_validation_split.outputs['output_gcs_path'], '--hptune', 'True'\n",
    "    ]\n",
    "\n",
    "    job_dir = '{}/{}/{}'.format(gcs_root, 'jobdir/hypertune',\n",
    "                                kfp.dsl.RUN_ID_PLACEHOLDER)\n",
    "\n",
    "#    hypertune = # TO DO: Use the mlengine_train_op\n",
    "    hypertune = mlengine_train_op(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        master_image_uri=TRAINER_IMAGE,\n",
    "        job_dir=job_dir,\n",
    "        args=tune_args,\n",
    "        training_input=hypertune_settings)\n",
    "\n",
    "    # Retrieve the best trial\n",
    "    get_best_trial = retrieve_best_run_op(\n",
    "            project_id, hypertune.outputs['job_id'])\n",
    "\n",
    "    # Train the model on a combined training and validation datasets\n",
    "    job_dir = '{}/{}/{}'.format(gcs_root, 'jobdir', kfp.dsl.RUN_ID_PLACEHOLDER)\n",
    "\n",
    "    train_args = [\n",
    "        '--training_dataset_path',\n",
    "        create_training_split.outputs['output_gcs_path'],\n",
    "        '--validation_dataset_path',\n",
    "        create_validation_split.outputs['output_gcs_path'], '--alpha',\n",
    "        get_best_trial.outputs['alpha'], '--max_iter',\n",
    "        get_best_trial.outputs['max_iter'], '--hptune', 'False'\n",
    "    ]\n",
    "\n",
    "#    train_model = # TO DO: Use the mlengine_train_op\n",
    "    train_model = mlengine_train_op(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        master_image_uri=TRAINER_IMAGE,\n",
    "        job_dir=job_dir,\n",
    "        args=train_args)\n",
    "\n",
    "    # Evaluate the model on the testing split\n",
    "    eval_model = evaluate_model_op(\n",
    "        dataset_path=str(create_testing_split.outputs['output_gcs_path']),\n",
    "        model_path=str(train_model.outputs['job_dir']),\n",
    "        metric_name=evaluation_metric_name)\n",
    "\n",
    "    # Deploy the model if the primary metric is better than threshold\n",
    "    with kfp.dsl.Condition(eval_model.outputs['metric_value'] > evaluation_metric_threshold):\n",
    "        deploy_model = mlengine_deploy_op(\n",
    "        model_uri=train_model.outputs['job_dir'],\n",
    "        project_id=project_id,\n",
    "        model_id=model_id,\n",
    "        version_id=version_id,\n",
    "        runtime_version=RUNTIME_VERSION,\n",
    "        python_version=PYTHON_VERSION,\n",
    "        replace_existing_version=replace_existing_version)\n",
    "\n",
    "    # Configure the pipeline to run using the service account defined\n",
    "    # in the user-gcp-sa k8s secret\n",
    "    if USE_KFP_SA == 'True':\n",
    "        kfp.dsl.get_pipeline_conf().add_op_transformer(\n",
    "              use_gcp_secret('user-gcp-sa'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The custom components execute in a container image defined in `base_image/Dockerfile`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "RUN pip install -U fire scikit-learn==0.20.4 pandas==0.24.2 kfp==0.2.5\n"
     ]
    }
   ],
   "source": [
    "!cat base_image/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training step in the pipeline employes the AI Platform Training component to schedule a  AI Platform Training job in a custom training container. The custom training image is defined in `trainer_image/Dockerfile`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
      "WORKDIR /app\n",
      "COPY train.py .\n",
      "\n",
      "ENTRYPOINT [\"python\", \"train.py\"]\n"
     ]
    }
   ],
   "source": [
    "!cat trainer_image/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and deploying the pipeline\n",
    "\n",
    "Before deploying to AI Platform Pipelines, the pipeline DSL has to be compiled into a pipeline runtime format, also refered to as a pipeline package.  The runtime format is based on [Argo Workflow](https://github.com/argoproj/argo), which is expressed in YAML. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure environment settings\n",
    "\n",
    "Update  the below constants  with the settings reflecting your lab environment. \n",
    "\n",
    "- `REGION` - the compute region for AI Platform Training and Prediction\n",
    "- `ARTIFACT_STORE` - the GCS bucket created during installation of AI Platform Pipelines. The bucket name will be similar to `qwiklabs-gcp-xx-xxxxxxx-kubeflowpipelines-default`.\n",
    "- `ENDPOINT` - set the `ENDPOINT` constant to the endpoint to your AI Platform Pipelines instance. Then endpoint to the AI Platform Pipelines instance can be found on the [AI Platform Pipelines](https://console.cloud.google.com/ai-platform/pipelines/clusters) page in the Google Cloud Console.\n",
    "\n",
    "1. Open the **SETTINGS** for your instance\n",
    "2. Use the value of the `host` variable in the **Connect to this Kubeflow Pipelines instance from a Python client via Kubeflow Pipelines SKD** section of the **SETTINGS** window.\n",
    "\n",
    "Run gsutil ls without URLs to list all of the Cloud Storage buckets under your default project ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://artifacts.qwiklabs-gcp-00-0a441c097ac4.appspot.com/\n",
      "gs://qwiklabs-gcp-00-0a441c097ac4-kubeflowpipelines-default/\n",
      "gs://qwiklabs-gcp-00-0a441c097ac4_cloudbuild/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HINT:** \n",
    "\n",
    "For **ENDPOINT**, use the value of the `host` variable in the **Connect to this Kubeflow Pipelines instance from a Python client via Kubeflow Pipelines SDK** section of the **SETTINGS** window.\n",
    "\n",
    "For **ARTIFACT_STORE_URI**, copy the bucket name which starts with the qwiklabs-gcp-xx-xxxxxxx-kubeflowpipelines-default prefix from the previous cell output. Your copied value should look like **'gs://qwiklabs-gcp-xx-xxxxxxx-kubeflowpipelines-default'**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "ENDPOINT = '221ed162182180b9-dot-us-central1.pipelines.googleusercontent.com' # TO DO: REPLACE WITH YOUR ENDPOINT\n",
    "#import kfp\n",
    "#client = kfp.Client(host='https://221ed162182180b9-dot-us-central1.pipelines.googleusercontent.com')\n",
    "\n",
    "#ARTIFACT_STORE_URI = 'gs://qwiklabs-gcp-xx-xxxxxxx-kubeflowpipelines-default' # TO DO: REPLACE WITH YOUR ARTIFACT_STORE NAME \n",
    "ARTIFACT_STORE_URI = 'gs://qwiklabs-gcp-00-0a441c097ac4-kubeflowpipelines-default' # TO DO\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the trainer image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='trainer_image'\n",
    "TAG='latest'\n",
    "TRAINER_IMAGE='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, TAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Note**: Please ignore any **incompatibility ERROR** that may appear for the packages visions as it will not affect the lab's functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 2 file(s) totalling 3.4 KiB before compression.\n",
      "Uploading tarball of [trainer_image] to [gs://qwiklabs-gcp-00-0a441c097ac4_cloudbuild/source/1671387928.628736-05b8a794d86c454eaee63d34c8cb3caf.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/qwiklabs-gcp-00-0a441c097ac4/locations/global/builds/7a4dec99-512e-4670-8a46-a9a542aaa7f2].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/7a4dec99-512e-4670-8a46-a9a542aaa7f2?project=1051368008164 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"7a4dec99-512e-4670-8a46-a9a542aaa7f2\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://qwiklabs-gcp-00-0a441c097ac4_cloudbuild/source/1671387928.628736-05b8a794d86c454eaee63d34c8cb3caf.tgz#1671387928881028\n",
      "Copying gs://qwiklabs-gcp-00-0a441c097ac4_cloudbuild/source/1671387928.628736-05b8a794d86c454eaee63d34c8cb3caf.tgz#1671387928881028...\n",
      "/ [1 files][  1.7 KiB/  1.7 KiB]                                                \n",
      "Operation completed over 1 objects/1.7 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  6.144kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "846c0b181fff: Pulling fs layer\n",
      "6b26f31ec77f: Pulling fs layer\n",
      "70ac61453ec9: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "10b372877087: Pulling fs layer\n",
      "20f1b7652517: Pulling fs layer\n",
      "84df69c4ac6f: Pulling fs layer\n",
      "14561663854c: Pulling fs layer\n",
      "f761f8e31879: Pulling fs layer\n",
      "6a2db770d3ae: Pulling fs layer\n",
      "7fa63c4577cd: Pulling fs layer\n",
      "f465f4872229: Pulling fs layer\n",
      "2148dca31586: Pulling fs layer\n",
      "b0aa4801fb2c: Pulling fs layer\n",
      "2c0d807054bb: Pulling fs layer\n",
      "08702b4ebe48: Pulling fs layer\n",
      "fa5bfef63a60: Pulling fs layer\n",
      "63035871a55e: Pulling fs layer\n",
      "d726a7dc6b45: Pulling fs layer\n",
      "a9ade8b3a9c0: Pulling fs layer\n",
      "509960026bd7: Pulling fs layer\n",
      "08d3069ffb04: Pulling fs layer\n",
      "4f4fb700ef54: Waiting\n",
      "10b372877087: Waiting\n",
      "20f1b7652517: Waiting\n",
      "84df69c4ac6f: Waiting\n",
      "14561663854c: Waiting\n",
      "f761f8e31879: Waiting\n",
      "6a2db770d3ae: Waiting\n",
      "7fa63c4577cd: Waiting\n",
      "f465f4872229: Waiting\n",
      "2148dca31586: Waiting\n",
      "b0aa4801fb2c: Waiting\n",
      "2c0d807054bb: Waiting\n",
      "08702b4ebe48: Waiting\n",
      "fa5bfef63a60: Waiting\n",
      "63035871a55e: Waiting\n",
      "d726a7dc6b45: Waiting\n",
      "a9ade8b3a9c0: Waiting\n",
      "509960026bd7: Waiting\n",
      "08d3069ffb04: Waiting\n",
      "70ac61453ec9: Verifying Checksum\n",
      "70ac61453ec9: Download complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "846c0b181fff: Verifying Checksum\n",
      "846c0b181fff: Download complete\n",
      "6b26f31ec77f: Download complete\n",
      "84df69c4ac6f: Verifying Checksum\n",
      "84df69c4ac6f: Download complete\n",
      "14561663854c: Verifying Checksum\n",
      "14561663854c: Download complete\n",
      "f761f8e31879: Download complete\n",
      "20f1b7652517: Verifying Checksum\n",
      "20f1b7652517: Download complete\n",
      "7fa63c4577cd: Verifying Checksum\n",
      "7fa63c4577cd: Download complete\n",
      "f465f4872229: Verifying Checksum\n",
      "f465f4872229: Download complete\n",
      "2148dca31586: Download complete\n",
      "b0aa4801fb2c: Verifying Checksum\n",
      "b0aa4801fb2c: Download complete\n",
      "2c0d807054bb: Verifying Checksum\n",
      "2c0d807054bb: Download complete\n",
      "08702b4ebe48: Verifying Checksum\n",
      "08702b4ebe48: Download complete\n",
      "fa5bfef63a60: Verifying Checksum\n",
      "fa5bfef63a60: Download complete\n",
      "63035871a55e: Verifying Checksum\n",
      "63035871a55e: Download complete\n",
      "d726a7dc6b45: Verifying Checksum\n",
      "d726a7dc6b45: Download complete\n",
      "a9ade8b3a9c0: Verifying Checksum\n",
      "a9ade8b3a9c0: Download complete\n",
      "6a2db770d3ae: Verifying Checksum\n",
      "6a2db770d3ae: Download complete\n",
      "08d3069ffb04: Verifying Checksum\n",
      "08d3069ffb04: Download complete\n",
      "10b372877087: Verifying Checksum\n",
      "10b372877087: Download complete\n",
      "846c0b181fff: Pull complete\n",
      "6b26f31ec77f: Pull complete\n",
      "70ac61453ec9: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "509960026bd7: Verifying Checksum\n",
      "509960026bd7: Download complete\n",
      "10b372877087: Pull complete\n",
      "20f1b7652517: Pull complete\n",
      "84df69c4ac6f: Pull complete\n",
      "14561663854c: Pull complete\n",
      "f761f8e31879: Pull complete\n",
      "6a2db770d3ae: Pull complete\n",
      "7fa63c4577cd: Pull complete\n",
      "f465f4872229: Pull complete\n",
      "2148dca31586: Pull complete\n",
      "b0aa4801fb2c: Pull complete\n",
      "2c0d807054bb: Pull complete\n",
      "08702b4ebe48: Pull complete\n",
      "fa5bfef63a60: Pull complete\n",
      "63035871a55e: Pull complete\n",
      "d726a7dc6b45: Pull complete\n",
      "a9ade8b3a9c0: Pull complete\n",
      "509960026bd7: Pull complete\n",
      "08d3069ffb04: Pull complete\n",
      "Digest: sha256:6482b219907c8cca86843b87409c80e29a3092812a6232130b0e791250406710\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> cfb092e26086\n",
      "Step 2/5 : RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
      " ---> Running in c473f892f052\n",
      "Collecting fire\n",
      "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88.3/88.3 kB 4.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting scikit-learn==0.20.4\n",
      "  Downloading scikit_learn-0.20.4-cp37-cp37m-manylinux1_x86_64.whl (5.4 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.4/5.4 MB 46.9 MB/s eta 0:00:00\n",
      "Collecting pandas==0.24.2\n",
      "  Downloading pandas-0.24.2-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.1/10.1 MB 40.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.21.6)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.7.3)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2011k in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2022.6)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.16.0)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-2.1.1-py3-none-any.whl (6.2 kB)\n",
      "Building wheels for collected packages: fire, cloudml-hypertune\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116931 sha256=c56bfe654c00e5f161bbf5b5dae833d9b8d78b3accde0e8b62176ece740984ed\n",
      "  Stored in directory: /root/.cache/pip/wheels/20/e8/7b/003fc14f02f262dd4614aec55e41147c8012e3dad98c936b76\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3973 sha256=12421c1c768feecd50365199ad31de8909f6defab58dd3d558bf562ff352f2f4\n",
      "  Stored in directory: /root/.cache/pip/wheels/7c/fb/ed/cfc98e70373dfe12db85fffab293e3153162f63de2f6aa5473\n",
      "Successfully built fire cloudml-hypertune\n",
      "Installing collected packages: cloudml-hypertune, termcolor, scikit-learn, pandas, fire\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.0.2\n",
      "    Uninstalling scikit-learn-1.0.2:\n",
      "      Successfully uninstalled scikit-learn-1.0.2\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.3.5\n",
      "    Uninstalling pandas-1.3.5:\n",
      "      Successfully uninstalled pandas-1.3.5\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "visions 0.7.5 requires pandas>=0.25.3, but you have pandas 0.24.2 which is incompatible.\n",
      "statsmodels 0.13.5 requires pandas>=0.25, but you have pandas 0.24.2 which is incompatible.\n",
      "seaborn 0.12.1 requires pandas>=0.25, but you have pandas 0.24.2 which is incompatible.\n",
      "phik 0.12.3 requires pandas>=0.25.1, but you have pandas 0.24.2 which is incompatible.\n",
      "pandas-profiling 3.5.0 requires pandas!=1.4.0,<1.6,>1.1, but you have pandas 0.24.2 which is incompatible.\n",
      "\u001b[0mSuccessfully installed cloudml-hypertune-0.1.0.dev6 fire-0.5.0 pandas-0.24.2 scikit-learn-0.20.4 termcolor-2.1.1\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container c473f892f052\n",
      " ---> 769f326453fe\n",
      "Step 3/5 : WORKDIR /app\n",
      " ---> Running in 54757b8735ff\n",
      "Removing intermediate container 54757b8735ff\n",
      " ---> e0635970e9e4\n",
      "Step 4/5 : COPY train.py .\n",
      " ---> e9817eda0651\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in ba70ce030912\n",
      "Removing intermediate container ba70ce030912\n",
      " ---> 745361141e4f\n",
      "Successfully built 745361141e4f\n",
      "Successfully tagged gcr.io/qwiklabs-gcp-00-0a441c097ac4/trainer_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/qwiklabs-gcp-00-0a441c097ac4/trainer_image:latest\n",
      "The push refers to repository [gcr.io/qwiklabs-gcp-00-0a441c097ac4/trainer_image]\n",
      "49869c275a81: Preparing\n",
      "a68f0c7aae9c: Preparing\n",
      "90c3d9eb0716: Preparing\n",
      "62d9d2b4312e: Preparing\n",
      "e50966ec4b57: Preparing\n",
      "f247696800e5: Preparing\n",
      "ddee9d8b7cee: Preparing\n",
      "c44f596f270a: Preparing\n",
      "6f9167e2fb81: Preparing\n",
      "eaad2008b716: Preparing\n",
      "c8f34353fb2c: Preparing\n",
      "b62e2c51d6a4: Preparing\n",
      "a0a6a46c0b32: Preparing\n",
      "84386cea57d8: Preparing\n",
      "1abdc8696632: Preparing\n",
      "c344f9abc101: Preparing\n",
      "8b3aef1e7f2d: Preparing\n",
      "33c0527480cb: Preparing\n",
      "dfdcf5f0e41b: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "552f9d623e1d: Preparing\n",
      "8972785ded2b: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "40368375c0e1: Preparing\n",
      "4b49fbacbc2e: Preparing\n",
      "0002c93bdb37: Preparing\n",
      "84386cea57d8: Waiting\n",
      "1abdc8696632: Waiting\n",
      "c344f9abc101: Waiting\n",
      "8b3aef1e7f2d: Waiting\n",
      "33c0527480cb: Waiting\n",
      "dfdcf5f0e41b: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "552f9d623e1d: Waiting\n",
      "8972785ded2b: Waiting\n",
      "40368375c0e1: Waiting\n",
      "4b49fbacbc2e: Waiting\n",
      "0002c93bdb37: Waiting\n",
      "f247696800e5: Waiting\n",
      "ddee9d8b7cee: Waiting\n",
      "c44f596f270a: Waiting\n",
      "6f9167e2fb81: Waiting\n",
      "b62e2c51d6a4: Waiting\n",
      "a0a6a46c0b32: Waiting\n",
      "eaad2008b716: Waiting\n",
      "c8f34353fb2c: Waiting\n",
      "62d9d2b4312e: Layer already exists\n",
      "e50966ec4b57: Layer already exists\n",
      "f247696800e5: Layer already exists\n",
      "ddee9d8b7cee: Layer already exists\n",
      "6f9167e2fb81: Layer already exists\n",
      "c44f596f270a: Layer already exists\n",
      "eaad2008b716: Layer already exists\n",
      "c8f34353fb2c: Layer already exists\n",
      "a0a6a46c0b32: Layer already exists\n",
      "b62e2c51d6a4: Layer already exists\n",
      "84386cea57d8: Layer already exists\n",
      "c344f9abc101: Layer already exists\n",
      "1abdc8696632: Layer already exists\n",
      "8b3aef1e7f2d: Layer already exists\n",
      "33c0527480cb: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "dfdcf5f0e41b: Layer already exists\n",
      "552f9d623e1d: Layer already exists\n",
      "8972785ded2b: Layer already exists\n",
      "40368375c0e1: Layer already exists\n",
      "4b49fbacbc2e: Layer already exists\n",
      "0002c93bdb37: Layer already exists\n",
      "a68f0c7aae9c: Pushed\n",
      "49869c275a81: Pushed\n",
      "90c3d9eb0716: Pushed\n",
      "latest: digest: sha256:9de7fcb00a6221da7f313a4f7f1d29a93e99a0f6fe9dd63da771e7617ae8c148 size: 5754\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                      IMAGES                                                       STATUS\n",
      "7a4dec99-512e-4670-8a46-a9a542aaa7f2  2022-12-18T18:25:29+00:00  2M41S     gs://qwiklabs-gcp-00-0a441c097ac4_cloudbuild/source/1671387928.628736-05b8a794d86c454eaee63d34c8cb3caf.tgz  gcr.io/qwiklabs-gcp-00-0a441c097ac4/trainer_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --timeout 15m --tag $TRAINER_IMAGE trainer_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the base image for custom components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='base_image'\n",
    "TAG='latest'\n",
    "BASE_IMAGE='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, TAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "The following command results in an error message.\n",
    "\n",
    "```bash\n",
    "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
    "visions 0.7.5 requires pandas>=0.25.3, but you have pandas 0.24.2 which is incompatible.\n",
    "typer 0.7.0 requires click<9.0.0,>=7.1.1, but you have click 7.0 which is incompatible.\n",
    "statsmodels 0.13.5 requires pandas>=0.25, but you have pandas 0.24.2 which is incompatible.\n",
    "seaborn 0.12.1 requires pandas>=0.25, but you have pandas 0.24.2 which is incompatible.\n",
    "phik 0.12.3 requires pandas>=0.25.1, but you have pandas 0.24.2 which is incompatible.\n",
    "pandas-profiling 3.5.0 requires pandas!=1.4.0,<1.6,>1.1, but you have pandas 0.24.2 which is incompatible.\n",
    "gym 0.23.1 requires cloudpickle>=1.2.0, but you have cloudpickle 1.1.1 which is incompatible.\n",
    "google-cloud-bigquery 3.4.1 requires packaging<22.0.0dev,>=14.3, but you have packaging 22.0 which is incompatible.\n",
    "docker 6.0.1 requires urllib3>=1.26.0, but you have urllib3 1.24.3 which is incompatible.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 1 file(s) totalling 122 bytes before compression.\n",
      "Uploading tarball of [base_image] to [gs://qwiklabs-gcp-00-0a441c097ac4_cloudbuild/source/1671388187.536451-5ca5c375ca1344c2bf0fb4c563ac8dd0.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/qwiklabs-gcp-00-0a441c097ac4/locations/global/builds/4ff4fb07-cf61-403f-8f5f-7f622b346492].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/4ff4fb07-cf61-403f-8f5f-7f622b346492?project=1051368008164 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"4ff4fb07-cf61-403f-8f5f-7f622b346492\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://qwiklabs-gcp-00-0a441c097ac4_cloudbuild/source/1671388187.536451-5ca5c375ca1344c2bf0fb4c563ac8dd0.tgz#1671388187779129\n",
      "Copying gs://qwiklabs-gcp-00-0a441c097ac4_cloudbuild/source/1671388187.536451-5ca5c375ca1344c2bf0fb4c563ac8dd0.tgz#1671388187779129...\n",
      "/ [1 files][  286.0 B/  286.0 B]                                                \n",
      "Operation completed over 1 objects/286.0 B.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  2.048kB\n",
      "Step 1/2 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "846c0b181fff: Pulling fs layer\n",
      "6b26f31ec77f: Pulling fs layer\n",
      "70ac61453ec9: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "10b372877087: Pulling fs layer\n",
      "20f1b7652517: Pulling fs layer\n",
      "84df69c4ac6f: Pulling fs layer\n",
      "14561663854c: Pulling fs layer\n",
      "f761f8e31879: Pulling fs layer\n",
      "6a2db770d3ae: Pulling fs layer\n",
      "7fa63c4577cd: Pulling fs layer\n",
      "f465f4872229: Pulling fs layer\n",
      "2148dca31586: Pulling fs layer\n",
      "b0aa4801fb2c: Pulling fs layer\n",
      "2c0d807054bb: Pulling fs layer\n",
      "08702b4ebe48: Pulling fs layer\n",
      "fa5bfef63a60: Pulling fs layer\n",
      "63035871a55e: Pulling fs layer\n",
      "d726a7dc6b45: Pulling fs layer\n",
      "a9ade8b3a9c0: Pulling fs layer\n",
      "509960026bd7: Pulling fs layer\n",
      "08d3069ffb04: Pulling fs layer\n",
      "4f4fb700ef54: Waiting\n",
      "10b372877087: Waiting\n",
      "20f1b7652517: Waiting\n",
      "84df69c4ac6f: Waiting\n",
      "14561663854c: Waiting\n",
      "f761f8e31879: Waiting\n",
      "6a2db770d3ae: Waiting\n",
      "7fa63c4577cd: Waiting\n",
      "2148dca31586: Waiting\n",
      "b0aa4801fb2c: Waiting\n",
      "2c0d807054bb: Waiting\n",
      "08702b4ebe48: Waiting\n",
      "fa5bfef63a60: Waiting\n",
      "63035871a55e: Waiting\n",
      "d726a7dc6b45: Waiting\n",
      "a9ade8b3a9c0: Waiting\n",
      "509960026bd7: Waiting\n",
      "f465f4872229: Waiting\n",
      "08d3069ffb04: Waiting\n",
      "70ac61453ec9: Verifying Checksum\n",
      "70ac61453ec9: Download complete\n",
      "4f4fb700ef54: Download complete\n",
      "846c0b181fff: Verifying Checksum\n",
      "846c0b181fff: Download complete\n",
      "6b26f31ec77f: Verifying Checksum\n",
      "6b26f31ec77f: Download complete\n",
      "84df69c4ac6f: Verifying Checksum\n",
      "84df69c4ac6f: Download complete\n",
      "14561663854c: Verifying Checksum\n",
      "14561663854c: Download complete\n",
      "f761f8e31879: Verifying Checksum\n",
      "f761f8e31879: Download complete\n",
      "20f1b7652517: Verifying Checksum\n",
      "20f1b7652517: Download complete\n",
      "7fa63c4577cd: Download complete\n",
      "f465f4872229: Verifying Checksum\n",
      "f465f4872229: Download complete\n",
      "2148dca31586: Verifying Checksum\n",
      "2148dca31586: Download complete\n",
      "b0aa4801fb2c: Verifying Checksum\n",
      "b0aa4801fb2c: Download complete\n",
      "2c0d807054bb: Verifying Checksum\n",
      "2c0d807054bb: Download complete\n",
      "6a2db770d3ae: Verifying Checksum\n",
      "6a2db770d3ae: Download complete\n",
      "08702b4ebe48: Verifying Checksum\n",
      "08702b4ebe48: Download complete\n",
      "fa5bfef63a60: Verifying Checksum\n",
      "fa5bfef63a60: Download complete\n",
      "63035871a55e: Verifying Checksum\n",
      "63035871a55e: Download complete\n",
      "d726a7dc6b45: Verifying Checksum\n",
      "d726a7dc6b45: Download complete\n",
      "a9ade8b3a9c0: Verifying Checksum\n",
      "a9ade8b3a9c0: Download complete\n",
      "08d3069ffb04: Verifying Checksum\n",
      "08d3069ffb04: Download complete\n",
      "10b372877087: Verifying Checksum\n",
      "10b372877087: Download complete\n",
      "846c0b181fff: Pull complete\n",
      "6b26f31ec77f: Pull complete\n",
      "70ac61453ec9: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "509960026bd7: Verifying Checksum\n",
      "509960026bd7: Download complete\n",
      "10b372877087: Pull complete\n",
      "20f1b7652517: Pull complete\n",
      "84df69c4ac6f: Pull complete\n",
      "14561663854c: Pull complete\n",
      "f761f8e31879: Pull complete\n",
      "6a2db770d3ae: Pull complete\n",
      "7fa63c4577cd: Pull complete\n",
      "f465f4872229: Pull complete\n",
      "2148dca31586: Pull complete\n",
      "b0aa4801fb2c: Pull complete\n",
      "2c0d807054bb: Pull complete\n",
      "08702b4ebe48: Pull complete\n",
      "fa5bfef63a60: Pull complete\n",
      "63035871a55e: Pull complete\n",
      "d726a7dc6b45: Pull complete\n",
      "a9ade8b3a9c0: Pull complete\n",
      "509960026bd7: Pull complete\n",
      "08d3069ffb04: Pull complete\n",
      "Digest: sha256:6482b219907c8cca86843b87409c80e29a3092812a6232130b0e791250406710\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> cfb092e26086\n",
      "Step 2/2 : RUN pip install -U fire scikit-learn==0.20.4 pandas==0.24.2 kfp==0.2.5\n",
      " ---> Running in 209cc511a2de\n",
      "Collecting fire\n",
      "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88.3/88.3 kB 4.8 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting scikit-learn==0.20.4\n",
      "  Downloading scikit_learn-0.20.4-cp37-cp37m-manylinux1_x86_64.whl (5.4 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.4/5.4 MB 45.0 MB/s eta 0:00:00\n",
      "Collecting pandas==0.24.2\n",
      "  Downloading pandas-0.24.2-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.1/10.1 MB 52.0 MB/s eta 0:00:00\n",
      "Collecting kfp==0.2.5\n",
      "  Downloading kfp-0.2.5.tar.gz (116 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 116.3/116.3 kB 17.3 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: scipy>=0.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.21.6)\n",
      "Requirement already satisfied: pytz>=2011k in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2022.6)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2.8.2)\n",
      "Collecting urllib3<1.25,>=1.15\n",
      "  Downloading urllib3-1.24.3-py2.py3-none-any.whl (118 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 118.8/118.8 kB 18.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (1.16.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (2022.12.7)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (6.0)\n",
      "Requirement already satisfied: google-cloud-storage>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (2.7.0)\n",
      "Collecting kubernetes<=10.0.0,>=8.0.0\n",
      "  Downloading kubernetes-10.0.0-py2.py3-none-any.whl (1.5 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 66.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: PyJWT>=1.6.4 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (2.6.0)\n",
      "Requirement already satisfied: cryptography>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (38.0.2)\n",
      "Requirement already satisfied: google-auth>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (2.15.0)\n",
      "Collecting requests_toolbelt>=0.8.0\n",
      "  Downloading requests_toolbelt-0.10.1-py2.py3-none-any.whl (54 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.5/54.5 kB 8.9 MB/s eta 0:00:00\n",
      "Collecting cloudpickle==1.1.1\n",
      "  Downloading cloudpickle-1.1.1-py2.py3-none-any.whl (17 kB)\n",
      "Collecting kfp-server-api<=0.1.40,>=0.1.18\n",
      "  Downloading kfp-server-api-0.1.40.tar.gz (38 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting argo-models==2.2.1a\n",
      "  Downloading argo-models-2.2.1a0.tar.gz (28 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: jsonschema>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (4.17.3)\n",
      "Collecting tabulate==0.8.3\n",
      "  Downloading tabulate-0.8.3.tar.gz (46 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 46.2/46.2 kB 6.9 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting click==7.0\n",
      "  Downloading Click-7.0-py2.py3-none-any.whl (81 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.3/81.3 kB 13.4 MB/s eta 0:00:00\n",
      "Collecting Deprecated\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting strip-hints\n",
      "  Downloading strip-hints-0.1.10.tar.gz (29 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-2.1.1-py3-none-any.whl (6.2 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.7/site-packages (from cryptography>=2.4.2->kfp==0.2.5) (1.15.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (5.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (0.2.8)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.2.5) (2.28.1)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.2.5) (1.34.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.2.5) (2.3.2)\n",
      "Requirement already satisfied: google-resumable-media>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.2.5) (2.4.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (5.1.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (5.10.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (22.1.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (4.4.0)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (1.3.10)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (0.19.2)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (1.4.2)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (65.5.1)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (1.3.1)\n",
      "Collecting wrapt<2,>=1.10\n",
      "  Downloading wrapt-1.14.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (75 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75.2/75.2 kB 13.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints->kfp==0.2.5) (0.38.4)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.12->cryptography>=2.4.2->kfp==0.2.5) (2.21)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage>=1.13.0->kfp==0.2.5) (1.57.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /opt/conda/lib/python3.7/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage>=1.13.0->kfp==0.2.5) (3.19.6)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media>=2.3.2->google-cloud-storage>=1.13.0->kfp==0.2.5) (1.5.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.7/site-packages (from importlib-resources>=1.4.0->jsonschema>=3.0.1->kfp==0.2.5) (3.11.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.6.1->kfp==0.2.5) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (3.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (3.2.2)\n",
      "Building wheels for collected packages: kfp, argo-models, tabulate, fire, kfp-server-api, strip-hints\n",
      "  Building wheel for kfp (setup.py): started\n",
      "  Building wheel for kfp (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp: filename=kfp-0.2.5-py3-none-any.whl size=159944 sha256=14947aa8a8d86ef7293d44e8ccd9325caae1ded39a631c40fcf68e6982f5c844\n",
      "  Stored in directory: /root/.cache/pip/wheels/62/ca/17/73ea6563f26005b47948f11cd16408d3373813b57084604400\n",
      "  Building wheel for argo-models (setup.py): started\n",
      "  Building wheel for argo-models (setup.py): finished with status 'done'\n",
      "  Created wheel for argo-models: filename=argo_models-2.2.1a0-py3-none-any.whl size=57284 sha256=6d1e766d6d4c95d2cb92e9b2d269df45b4989717d4bf731d5491ce909b37005d\n",
      "  Stored in directory: /root/.cache/pip/wheels/09/3a/24/c766c0346bcbef9084b83a4e5ef37e152d1e3cb7253f6f4259\n",
      "  Building wheel for tabulate (setup.py): started\n",
      "  Building wheel for tabulate (setup.py): finished with status 'done'\n",
      "  Created wheel for tabulate: filename=tabulate-0.8.3-py3-none-any.whl size=23375 sha256=c4bf29b040c009bc12736e6bb24afcf39cd881612a85dfd88ddd6728bb246365\n",
      "  Stored in directory: /root/.cache/pip/wheels/3e/91/ca/dc3638285fb8a32892bfba8f4c903d2fc623011c69d276be22\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116931 sha256=1ff5a2bc8fff951fe0f8fd167aa3098a943ef254a195d79dece5cb60bb6684a6\n",
      "  Stored in directory: /root/.cache/pip/wheels/20/e8/7b/003fc14f02f262dd4614aec55e41147c8012e3dad98c936b76\n",
      "  Building wheel for kfp-server-api (setup.py): started\n",
      "  Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp-server-api: filename=kfp_server_api-0.1.40-py3-none-any.whl size=102440 sha256=e5366a5616ed1417269a6cc53c6372f025100efb4f7fbc1245bb3dfa6587d71c\n",
      "  Stored in directory: /root/.cache/pip/wheels/0e/60/b3/6de4595a7bdcfe4c7ab7b7283c404c2584b78bca0a43600da4\n",
      "  Building wheel for strip-hints (setup.py): started\n",
      "  Building wheel for strip-hints (setup.py): finished with status 'done'\n",
      "  Created wheel for strip-hints: filename=strip_hints-0.1.10-py2.py3-none-any.whl size=22283 sha256=1bc2f253cf4b6c074ab488661d375f7f1bb765e7f2951561265edc496edb9eca\n",
      "  Stored in directory: /root/.cache/pip/wheels/5a/4e/89/a4493443c6aadb8a38e9709610efc3bfafaea3a4108df4c112\n",
      "Successfully built kfp argo-models tabulate fire kfp-server-api strip-hints\n",
      "Installing collected packages: tabulate, cloudpickle, wrapt, urllib3, termcolor, strip-hints, click, scikit-learn, pandas, kfp-server-api, fire, Deprecated, requests_toolbelt, kubernetes, argo-models, kfp\n",
      "  Attempting uninstall: tabulate\n",
      "    Found existing installation: tabulate 0.9.0\n",
      "    Uninstalling tabulate-0.9.0:\n",
      "      Successfully uninstalled tabulate-0.9.0\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 2.2.0\n",
      "    Uninstalling cloudpickle-2.2.0:\n",
      "      Successfully uninstalled cloudpickle-2.2.0\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.13\n",
      "    Uninstalling urllib3-1.26.13:\n",
      "      Successfully uninstalled urllib3-1.26.13\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.1.3\n",
      "    Uninstalling click-8.1.3:\n",
      "      Successfully uninstalled click-8.1.3\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.0.2\n",
      "    Uninstalling scikit-learn-1.0.2:\n",
      "      Successfully uninstalled scikit-learn-1.0.2\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.3.5\n",
      "    Uninstalling pandas-1.3.5:\n",
      "      Successfully uninstalled pandas-1.3.5\n",
      "  Attempting uninstall: kubernetes\n",
      "    Found existing installation: kubernetes 25.3.0\n",
      "    Uninstalling kubernetes-25.3.0:\n",
      "      Successfully uninstalled kubernetes-25.3.0\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "visions 0.7.5 requires pandas>=0.25.3, but you have pandas 0.24.2 which is incompatible.\n",
      "typer 0.7.0 requires click<9.0.0,>=7.1.1, but you have click 7.0 which is incompatible.\n",
      "statsmodels 0.13.5 requires pandas>=0.25, but you have pandas 0.24.2 which is incompatible.\n",
      "seaborn 0.12.1 requires pandas>=0.25, but you have pandas 0.24.2 which is incompatible.\n",
      "phik 0.12.3 requires pandas>=0.25.1, but you have pandas 0.24.2 which is incompatible.\n",
      "pandas-profiling 3.5.0 requires pandas!=1.4.0,<1.6,>1.1, but you have pandas 0.24.2 which is incompatible.\n",
      "gym 0.23.1 requires cloudpickle>=1.2.0, but you have cloudpickle 1.1.1 which is incompatible.\n",
      "google-cloud-bigquery 3.4.1 requires packaging<22.0.0dev,>=14.3, but you have packaging 22.0 which is incompatible.\n",
      "docker 6.0.1 requires urllib3>=1.26.0, but you have urllib3 1.24.3 which is incompatible.\n",
      "\u001b[0mSuccessfully installed Deprecated-1.2.13 argo-models-2.2.1a0 click-7.0 cloudpickle-1.1.1 fire-0.5.0 kfp-0.2.5 kfp-server-api-0.1.40 kubernetes-10.0.0 pandas-0.24.2 requests_toolbelt-0.10.1 scikit-learn-0.20.4 strip-hints-0.1.10 tabulate-0.8.3 termcolor-2.1.1 urllib3-1.24.3 wrapt-1.14.1\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 209cc511a2de\n",
      " ---> 1f533089b91c\n",
      "Successfully built 1f533089b91c\n",
      "Successfully tagged gcr.io/qwiklabs-gcp-00-0a441c097ac4/base_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/qwiklabs-gcp-00-0a441c097ac4/base_image:latest\n",
      "The push refers to repository [gcr.io/qwiklabs-gcp-00-0a441c097ac4/base_image]\n",
      "7806e65ac3ad: Preparing\n",
      "62d9d2b4312e: Preparing\n",
      "e50966ec4b57: Preparing\n",
      "f247696800e5: Preparing\n",
      "ddee9d8b7cee: Preparing\n",
      "c44f596f270a: Preparing\n",
      "6f9167e2fb81: Preparing\n",
      "eaad2008b716: Preparing\n",
      "c8f34353fb2c: Preparing\n",
      "b62e2c51d6a4: Preparing\n",
      "a0a6a46c0b32: Preparing\n",
      "84386cea57d8: Preparing\n",
      "1abdc8696632: Preparing\n",
      "c344f9abc101: Preparing\n",
      "8b3aef1e7f2d: Preparing\n",
      "33c0527480cb: Preparing\n",
      "dfdcf5f0e41b: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "552f9d623e1d: Preparing\n",
      "8972785ded2b: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "40368375c0e1: Preparing\n",
      "4b49fbacbc2e: Preparing\n",
      "0002c93bdb37: Preparing\n",
      "1abdc8696632: Waiting\n",
      "c344f9abc101: Waiting\n",
      "8b3aef1e7f2d: Waiting\n",
      "33c0527480cb: Waiting\n",
      "dfdcf5f0e41b: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "552f9d623e1d: Waiting\n",
      "8972785ded2b: Waiting\n",
      "40368375c0e1: Waiting\n",
      "4b49fbacbc2e: Waiting\n",
      "0002c93bdb37: Waiting\n",
      "c44f596f270a: Waiting\n",
      "6f9167e2fb81: Waiting\n",
      "eaad2008b716: Waiting\n",
      "c8f34353fb2c: Waiting\n",
      "84386cea57d8: Waiting\n",
      "b62e2c51d6a4: Waiting\n",
      "a0a6a46c0b32: Waiting\n",
      "ddee9d8b7cee: Layer already exists\n",
      "f247696800e5: Layer already exists\n",
      "62d9d2b4312e: Layer already exists\n",
      "e50966ec4b57: Layer already exists\n",
      "c44f596f270a: Layer already exists\n",
      "6f9167e2fb81: Layer already exists\n",
      "c8f34353fb2c: Layer already exists\n",
      "eaad2008b716: Layer already exists\n",
      "b62e2c51d6a4: Layer already exists\n",
      "84386cea57d8: Layer already exists\n",
      "1abdc8696632: Layer already exists\n",
      "a0a6a46c0b32: Layer already exists\n",
      "8b3aef1e7f2d: Layer already exists\n",
      "33c0527480cb: Layer already exists\n",
      "c344f9abc101: Layer already exists\n",
      "dfdcf5f0e41b: Layer already exists\n",
      "40368375c0e1: Layer already exists\n",
      "8972785ded2b: Layer already exists\n",
      "552f9d623e1d: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "0002c93bdb37: Layer already exists\n",
      "4b49fbacbc2e: Layer already exists\n",
      "7806e65ac3ad: Pushed\n",
      "latest: digest: sha256:875b6dc233f876d7bdfd428cfc6b1793b5034b2d99500f5277fa3843e304d16e size: 5339\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                      IMAGES                                                    STATUS\n",
      "4ff4fb07-cf61-403f-8f5f-7f622b346492  2022-12-18T18:29:47+00:00  3M1S      gs://qwiklabs-gcp-00-0a441c097ac4_cloudbuild/source/1671388187.536451-5ca5c375ca1344c2bf0fb4c563ac8dd0.tgz  gcr.io/qwiklabs-gcp-00-0a441c097ac4/base_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --timeout 15m --tag $BASE_IMAGE base_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the pipeline\n",
    "\n",
    "You can compile the DSL using an API from the **KFP SDK** or using the **KFP** compiler.\n",
    "\n",
    "To compile the pipeline DSL using the **KFP** compiler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the pipeline's compile time settings\n",
    "\n",
    "The pipeline can run using a security context of the GKE default node pool's service account or the service account defined in the `user-gcp-sa` secret of the Kubernetes namespace hosting KFP. If you want to use the `user-gcp-sa` service account you change the value of `USE_KFP_SA` to `True`.\n",
    "\n",
    "Note that the default AI Platform Pipelines configuration does not define the `user-gcp-sa` secret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: USE_KFP_SA=False\n",
      "env: BASE_IMAGE=gcr.io/qwiklabs-gcp-00-0a441c097ac4/base_image:latest\n",
      "env: TRAINER_IMAGE=gcr.io/qwiklabs-gcp-00-0a441c097ac4/trainer_image:latest\n",
      "env: COMPONENT_URL_SEARCH_PREFIX=https://raw.githubusercontent.com/kubeflow/pipelines/0.2.5/components/gcp/\n",
      "env: RUNTIME_VERSION=1.15\n",
      "env: PYTHON_VERSION=3.7\n"
     ]
    }
   ],
   "source": [
    "USE_KFP_SA = False\n",
    "\n",
    "COMPONENT_URL_SEARCH_PREFIX = 'https://raw.githubusercontent.com/kubeflow/pipelines/0.2.5/components/gcp/'\n",
    "RUNTIME_VERSION = '1.15'\n",
    "PYTHON_VERSION = '3.7'\n",
    "\n",
    "%env USE_KFP_SA={USE_KFP_SA}\n",
    "%env BASE_IMAGE={BASE_IMAGE}\n",
    "%env TRAINER_IMAGE={TRAINER_IMAGE}\n",
    "%env COMPONENT_URL_SEARCH_PREFIX={COMPONENT_URL_SEARCH_PREFIX}\n",
    "%env RUNTIME_VERSION={RUNTIME_VERSION}\n",
    "%env PYTHON_VERSION={PYTHON_VERSION}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the CLI compiler to compile the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Compile the `covertype_training_pipeline.py` with the `dsl-compile` command line:\n",
    "\n",
    "<ql-infobox><b>NOTE:</b> If you need help, you may take a look at the complete solution by navigating to **mlops-on-gcp > workshops > kfp-caip-sklearn > lab-02-kfp-pipeline** and opening **lab-02.ipynb**.\n",
    "</ql-infobox>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dsl-compile --py pipeline/covertype_training_pipeline.py --output covertype_training_pipeline.yaml  # TO DO: Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is the `covertype_training_pipeline.yaml` file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"apiVersion\": |-\n",
      "  argoproj.io/v1alpha1\n",
      "\"kind\": |-\n",
      "  Workflow\n",
      "\"metadata\":\n",
      "  \"annotations\":\n",
      "    \"pipelines.kubeflow.org/pipeline_spec\": |-\n",
      "      {\"description\": \"The pipeline training and deploying the Covertype classifierpipeline_yaml\", \"inputs\": [{\"name\": \"project_id\"}, {\"name\": \"region\"}, {\"name\": \"source_table_name\"}, {\"name\": \"gcs_root\"}, {\"name\": \"dataset_id\"}, {\"name\": \"evaluation_metric_name\"}, {\"name\": \"evaluation_metric_threshold\"}, {\"name\": \"model_id\"}, {\"name\": \"version_id\"}, {\"name\": \"replace_existing_version\"}, {\"default\": \"\\n{\\n    \\\"hyperparameters\\\":  {\\n        \\\"goal\\\": \\\"MAXIMIZE\\\",\\n        \\\"maxTrials\\\": 6,\\n        \\\"maxParallelTrials\\\": 3,\\n        \\\"hyperparameterMetricTag\\\": \\\"accuracy\\\",\\n        \\\"enableTrialEarlyStopping\\\": True,\\n        \\\"params\\\": [\\n            {\\n                \\\"parameterName\\\": \\\"max_iter\\\",\\n                \\\"type\\\": \\\"DISCRETE\\\",\\n                \\\"discreteValues\\\": [500, 1000]\\n            },\\n            {\\n                \\\"parameterName\\\": \\\"alpha\\\",\\n                \\\"type\\\": \\\"DOUBLE\\\",\\n                \\\"minValue\\\": 0.0001,\\n                \\\"maxValue\\\": 0.001,\\n                \\\"scaleType\\\": \\\"UNIT_LINEAR_SCALE\\\"\\n            }\\n        ]\\n    }\\n}\\n\", \"name\": \"hypertune_settings\", \"optional\": true}, {\"default\": \"US\", \"name\": \"dataset_location\", \"optional\": true}], \"name\": \"Covertype Classifier Training\"}\n",
      "  \"generateName\": |-\n",
      "    covertype-classifier-training-\n"
     ]
    }
   ],
   "source": [
    "!head covertype_training_pipeline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the command fields in the pipeline YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -i 's/\\\"command\\\": \\[\\]/\\\"command\\\": \\[python, -u, -m, kfp_component.launcher\\]/g' covertype_training_pipeline.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      \"command\": [python, -u, -m, kfp_component.launcher]\n",
      "      \"command\": [python, -u, -m, kfp_component.launcher]\n",
      "      \"command\": [python, -u, -m, kfp_component.launcher]\n",
      "      \"command\": [python, -u, -m, kfp_component.launcher]\n",
      "      \"command\": [python, -u, -m, kfp_component.launcher]\n",
      "      \"command\": [python, -u, -m, kfp_component.launcher]\n"
     ]
    }
   ],
   "source": [
    "!cat covertype_training_pipeline.yaml | grep \"component.launcher\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see 6 lines in the output that were modified by the sed command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the pipeline package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Upload the pipeline to the Kubeflow cluster using the `kfp` command line:\n",
    "\n",
    "<ql-infobox><b>NOTE:</b> If you need help, you may take a look at the complete solution by navigating to **mlops-on-gcp > workshops > kfp-caip-sklearn > lab-02-kfp-pipeline** and opening **lab-02.ipynb**.\n",
    "</ql-infobox>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500)\n",
      "Reason: Internal Server Error\n",
      "HTTP response headers: HTTPHeaderDict({'Content-Type': 'text/plain; charset=utf-8', 'Date': 'Sun, 18 Dec 2022 18:33:02 GMT', 'Vary': 'Origin', 'X-Content-Type-Options': 'nosniff', 'X-Frame-Options': 'SAMEORIGIN', 'X-Powered-By': 'Express', 'X-Xss-Protection': '0', 'Transfer-Encoding': 'chunked', 'Set-Cookie': 'S=cloud_datalab_tunnel=_FB--d11o2KIv-gQjTbTunGPxeX0onUVcBV91Iku9fM; Path=/; Max-Age=3600'})\n",
      "HTTP response body: {\"error_message\":\"Error creating pipeline: Create pipeline failed: Already exist error: Failed to create a new pipeline. The name covertype_continuous_training already exist. Please specify a new name.\",\"error_details\":\"Error creating pipeline: Create pipeline failed: Already exist error: Failed to create a new pipeline. The name covertype_continuous_training already exist. Please specify a new name.\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PIPELINE_NAME='covertype_continuous_training'\n",
    "\n",
    "# TO DO: Your code goes here\n",
    "!kfp --endpoint $ENDPOINT pipeline upload \\\n",
    "-p $PIPELINE_NAME \\\n",
    "covertype_training_pipeline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting pipeline runs\n",
    "\n",
    "You can trigger pipeline runs using an API from the KFP SDK or using KFP CLI. To submit the run using KFP CLI, execute the following commands. Notice how the pipeline's parameters are passed to the pipeline run.\n",
    "\n",
    "### List the pipelines in AI Platform Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+------------------------------------------------+---------------------------+\n",
      "| Pipeline ID                          | Name                                           | Uploaded at               |\n",
      "+======================================+================================================+===========================+\n",
      "| 48fb3028-4ea1-4339-a850-599d59de2e94 | covertype_continuous_training                  | 2022-12-18T18:29:42+00:00 |\n",
      "+--------------------------------------+------------------------------------------------+---------------------------+\n",
      "| fb5248bc-77a9-4581-949f-53e1ed018465 | [Tutorial] DSL - Control structures            | 2022-12-18T17:47:56+00:00 |\n",
      "+--------------------------------------+------------------------------------------------+---------------------------+\n",
      "| 7cc9d880-b739-41be-9e24-86dddb23b44c | [Tutorial] Data passing in python components   | 2022-12-18T17:47:55+00:00 |\n",
      "+--------------------------------------+------------------------------------------------+---------------------------+\n",
      "| 7d29910e-7376-4106-851e-edca7bc2ea6e | [Demo] TFX - Taxi tip prediction model trainer | 2022-12-18T17:47:54+00:00 |\n",
      "+--------------------------------------+------------------------------------------------+---------------------------+\n",
      "| bbd9448c-ea89-4220-90ef-fa06062fb827 | [Demo] XGBoost - Iterative model training      | 2022-12-18T17:47:53+00:00 |\n",
      "+--------------------------------------+------------------------------------------------+---------------------------+\n"
     ]
    }
   ],
   "source": [
    "!kfp --endpoint $ENDPOINT pipeline list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit a run\n",
    "\n",
    "Find the ID of the `covertype_continuous_training` pipeline you uploaded in the previous step and update the value of `PIPELINE_ID` .\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "Q: What is PIPELINE_ID in this case?\n",
    "A: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_ID='0918568d-758c-46cf-9752-e04a4403cd84' # TO DO: REPLACE WITH YOUR PIPELINE ID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = 'Covertype_Classifier_Training'\n",
    "RUN_ID = 'Run_001'\n",
    "SOURCE_TABLE = 'covertype_dataset.covertype'\n",
    "DATASET_ID = 'splits'\n",
    "EVALUATION_METRIC = 'accuracy'\n",
    "EVALUATION_METRIC_THRESHOLD = '0.69'\n",
    "MODEL_ID = 'covertype_classifier'\n",
    "VERSION_ID = 'v01'\n",
    "REPLACE_EXISTING_VERSION = 'True'\n",
    "\n",
    "GCS_STAGING_PATH = '{}/staging'.format(ARTIFACT_STORE_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Run the pipeline using the `kfp` command line. Here are some of the variable\n",
    "you will have to use to pass to the pipeline:\n",
    "\n",
    "- EXPERIMENT_NAME is set to the experiment used to run the pipeline. You can choose any name you want. If the experiment does not exist it will be created by the command\n",
    "- RUN_ID is the name of the run. You can use an arbitrary name\n",
    "- PIPELINE_ID is the id of your pipeline. Use the value retrieved by the   `kfp pipeline list` command\n",
    "- GCS_STAGING_PATH is the URI to the Cloud Storage location used by the pipeline to store intermediate files. By default, it is set to the `staging` folder in your artifact store.\n",
    "- REGION is a compute region for AI Platform Training and Prediction.\n",
    "\n",
    "\n",
    "<ql-infobox><b>NOTE:</b> If you need help, you may take a look at the complete solution by navigating to **mlops-on-gcp > workshops > kfp-caip-sklearn > lab-02-kfp-pipeline** and opening **lab-02.ipynb**.\n",
    "</ql-infobox>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating experiment Covertype_Classifier_Training.\n",
      "(404)\n",
      "Reason: Not Found\n",
      "HTTP response headers: HTTPHeaderDict({'Content-Type': 'application/json', 'Date': 'Sun, 18 Dec 2022 18:33:08 GMT', 'Vary': 'Origin', 'X-Content-Type-Options': 'nosniff', 'X-Frame-Options': 'SAMEORIGIN', 'X-Powered-By': 'Express', 'X-Xss-Protection': '0', 'Transfer-Encoding': 'chunked', 'Set-Cookie': 'S=cloud_datalab_tunnel=_FB--d11o2KIv-gQjTbTunGPxeX0onUVcBV91Iku9fM; Path=/; Max-Age=3600'})\n",
      "HTTP response body: {\"error\":\"Validate create run request failed.: Get pipelineId failed.: ResourceNotFoundError: Pipeline 0918568d-758c-46cf-9752-e04a4403cd84 not found.\",\"code\":5,\"message\":\"Validate create run request failed.: Get pipelineId failed.: ResourceNotFoundError: Pipeline 0918568d-758c-46cf-9752-e04a4403cd84 not found.\",\"details\":[{\"@type\":\"type.googleapis.com/api.Error\",\"error_message\":\"Pipeline 0918568d-758c-46cf-9752-e04a4403cd84 not found.\",\"error_details\":\"Validate create run request failed.: Get pipelineId failed.: ResourceNotFoundError: Pipeline 0918568d-758c-46cf-9752-e04a4403cd84 not found.\"}]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Your code goes here\n",
    "!kfp --endpoint $ENDPOINT run submit \\\n",
    "-e $EXPERIMENT_NAME \\\n",
    "-r $RUN_ID \\\n",
    "-p $PIPELINE_ID \\\n",
    "project_id=$PROJECT_ID \\\n",
    "gcs_root=$GCS_STAGING_PATH \\\n",
    "region=$REGION \\\n",
    "source_table_name=$SOURCE_TABLE \\\n",
    "dataset_id=$DATASET_ID \\\n",
    "evaluation_metric_name=$EVALUATION_METRIC \\\n",
    "evaluation_metric_threshold=$EVALUATION_METRIC_THRESHOLD \\\n",
    "model_id=$MODEL_ID \\\n",
    "version_id=$VERSION_ID \\\n",
    "replace_existing_version=$REPLACE_EXISTING_VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring the run\n",
    "\n",
    "You can monitor the run using KFP UI. Follow the instructor who will walk you through the KFP UI and monitoring techniques.\n",
    "\n",
    "To access the KFP UI in your environment use the following URI:\n",
    "\n",
    "https://[ENDPOINT]\n",
    "\n",
    "\n",
    "**NOTE that your pipeline run may fail due to the bug in a BigQuery component that does not handle certain race conditions. If you observe the pipeline failure, re-run the last cell of the notebook to submit another pipeline run or retry the run from the KFP UI**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=-1>Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at [https://www.apache.org/licenses/LICENSE-2.0](https://www.apache.org/licenses/LICENSE-2.0)\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \\\"AS IS\\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and limitations under the License.</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
